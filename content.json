{"meta":{"title":"清弦小站","subtitle":"","description":"BLOG","author":"庞宇轩","url":"https://pyxblog.cn","root":"/"},"pages":[{"title":"","date":"2022-08-06T02:27:41.716Z","updated":"2022-08-06T02:27:41.716Z","comments":false,"path":"404.html","permalink":"https://pyxblog.cn/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有专栏","date":"2022-08-06T02:27:41.719Z","updated":"2022-08-06T02:27:41.719Z","comments":false,"path":"categories/index.html","permalink":"https://pyxblog.cn/categories/index.html","excerpt":"","text":""},{"title":"","date":"2022-08-07T07:10:13.565Z","updated":"2022-08-07T07:10:13.549Z","comments":false,"path":"about/index.html","permalink":"https://pyxblog.cn/about/index.html","excerpt":"","text":"庞宇轩 中国传媒大学 信息与通信工程学院 2019级广播电视工程专业"},{"title":"友情链接","date":"2022-08-06T02:27:41.720Z","updated":"2022-08-06T02:27:41.720Z","comments":true,"path":"friends/index.html","permalink":"https://pyxblog.cn/friends/index.html","excerpt":"提供一些日常学习生活中常用的网站&我的宝藏网站","text":"提供一些日常学习生活中常用的网站&我的宝藏网站 链接失效请直接在评论区反馈哦～"},{"title":"资源共享","date":"2022-08-06T02:27:41.720Z","updated":"2022-08-06T02:27:41.720Z","comments":true,"path":"share/index.html","permalink":"https://pyxblog.cn/share/index.html","excerpt":"提供一些日常学习生活中常用的网站&我的宝藏网站","text":"提供一些日常学习生活中常用的网站&我的宝藏网站 链接失效请直接在评论区反馈哦～"},{"title":"所有标签","date":"2022-08-06T02:27:41.720Z","updated":"2022-08-06T02:27:41.720Z","comments":false,"path":"tags/index.html","permalink":"https://pyxblog.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"scrapy爬虫实战03-爬取其他文件格式","slug":"python-spider-scrapy3","date":"2022-08-06T16:15:29.000Z","updated":"2022-08-06T16:18:39.303Z","comments":true,"path":"2022/08/07/python-spider-scrapy3/","link":"","permalink":"https://pyxblog.cn/2022/08/07/python-spider-scrapy3/","excerpt":"","text":"本次实战中，我们以图片为例，演示使用Scrapy框架爬取非文本内容的方法。 在前面两次的Scrapy框架爬虫实战中，已经对基础操作有了较为详细的解释说明，因此本次教程中的基础操作将不再过多赘述。 目标网站：传送门 爬虫编写 我们以CrawlSpider为工具进行爬取。 创建CrawlSpider爬虫 在命令行中创建爬虫： 1234cd zcoolscrapy startproject zcoolcd zcoolscrapy genspider -t crawl zcoolSpider https://www.zcool.com.cn/ 基础设置 进行一些常规化的基础设置，后续使用Scrapy框架时可以按照这样的思路直接往下进行。 创建start.py 创建start.py以实现在pycharm内运行Scrapy爬虫 12from scrapy import cmdlinecmdline.execute(&quot;scrapy crawl zcoolSpider&quot;.split(&quot; &quot;)) 关闭协议、设置ua 在settings.py中关闭那个君子协议，然后设置好自己的user-agent 1234567891011121314BOT_NAME = &#x27;zcool&#x27;SPIDER_MODULES = [&#x27;zcool.spiders&#x27;]NEWSPIDER_MODULE = &#x27;zcool.spiders&#x27;# Obey robots.txt rulesROBOTSTXT_OBEY = False# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;, &#x27;Accept-Language&#x27;: &#x27;en&#x27;, &#x27;User-Agent&#x27; : &#x27;我的user-agent&#x27;&#125; 设置初始页面 设置一下zcoolSpider.py（就是爬虫文件）里的start_urls，本次实战中我们爬取的是“精选部分”，页面链接在这：传送门 123name = &#x27;zcoolSpider&#x27; allowed_domains = [&#x27;zcool.com.cn&#x27;] start_urls = [&#x27;https://www.zcool.com.cn/discover/0!3!0!0!0!!!!1!1!1&#x27;] 编写灵魂——rules规则 页码对应url 不难找到不同页码对应链接的规律： 均为https://www.zcool.com.cn/discover/0!3!0!0!0!!!!1!1!+页码的形式 规则（正则表达式）应该这样写： 1Rule(LinkExtractor(allow=r&#x27;.+0!3!0!0!0!!!!1!1!\\d+&#x27;),follow=True) 详情页 详情页的规则也很明显，均为https://www.zcool.com.cn/work/+一串字母+=.html 规则（正则表达式）应该这样写： 1Rule(LinkExtractor(allow=r&#x27;.+work/.+html&#x27;),follow=False,callback=&quot;parse_detail&quot;) 数据解析与存储 上面已经写好了rules，使crawlSpider有了自己找到每一个详情页的能力，接下来我们就处理这些详情页。 编写回调函数parse_details 由于每个详情页里都有很多张图，所以我们期望把每一页里的图放在同一个文件夹里，然后以那一页的标题为文件名，这样便于我们以后查看。因此，在回调函数中，我们需要获取的内容主要有两个：标题和图片链接 获取标题 12title = response.xpath(&quot;//div[@class=&#x27;details-contitle-box&#x27;]/h2/text()&quot;).getall() # getall返回列表title = &quot;&quot;.join(title).strip() # 用于将列表拼接并删掉首尾的空格 获取图片url 利用div标签的class属性，定位图片的链接 1image_urls = response.xpath(&quot;//div[@class=&#x27;photo-information-content&#x27;]/img/@src&quot;).getall() ps. 我们可以在插件XPath Helper中验证自己找的xpath路径是否正确，如图： 的确是可以成功获取url 编写items.py 12345import scrapyclass ZcoolItem(scrapy.Item): title = scrapy.Field() # 标题 image_urls = scrapy.Field() # 图片链接 images = scrapy.Field() # 图片本身 在zcoolSpider.py中调用items.py 12345678from ..items import ZcoolItem...class ZcoolspiderSpider(CrawlSpider): ... def parse_detail(self, response): ... item = ZcoolItem(title=title,image_urls=image_urls) return item 在setting.py中打开piplines，并编写文件存储路径 12345678import osIMAGES_STORE = os.path.join(os.path.dirname(os.path.dirname(__file__)),&#x27;images&#x27;)# Configure item pipelines# See https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; &#x27;zcool.pipelines.ZcoolPipeline&#x27;: 300,&#125; 其中os.path.dirname的作用是获取上层文件夹路径，__file__就是只这个文件本身，os.path.join则实现了将路径拼接的作用。 编写piplines.py 12345678910111213141516171819from scrapy.pipelines.images import ImagesPipelinefrom zcool import settings # 这是想调用settings.py里写的IMAGE_STOREimport osimport re # 正则表达式库class ZcoolPipeline(ImagesPipeline): def get_media_requests(self, item, info): media_requests = super(ZcoolPipeline, self).get_media_requests(item,info) for media_request in media_requests: media_request.item = item return media_requests def file_path(self, request, response=None, info=None, *, item=None): origin_path = super(ZcoolPipeline, self).file_path(request, response, info) # 先执行一遍原函数 title = request.item[&#x27;title&#x27;] title = re.sub(r&#x27;[\\\\/:\\*\\?&quot;&lt;&gt;\\|]&#x27;,&quot;&quot;,title) # 删除非法字符 save_path = os.path.join(settings.IMAGES_STORE,title) image_name = origin_path.replace(&quot;full/&quot;,&quot;&quot;) return os.path.join(save_path,image_name) 注意到上面的title = re.sub(r'[\\\\/:\\*\\?\"&lt;&gt;\\|]',\"\",title)一句中，因为我们想用详情页的标题作为文件夹名，但文件夹名中不可以出现这些字符：\\ / : * ? \" &lt; &gt; |，因此我们要用正则表达式的方法，把标题中的这些字符删除。 至此，我们编写完了本次实战的爬虫，运行可得结果如下： 最终代码参考： zcoolSpider.py 123456789101112131415161718192021import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom ..items import ZcoolItemclass ZcoolspiderSpider(CrawlSpider): name = &#x27;zcoolSpider&#x27; allowed_domains = [&#x27;zcool.com.cn&#x27;] start_urls = [&#x27;https://www.zcool.com.cn/discover/0!3!0!0!0!!!!1!1!1&#x27;] rules = ( Rule(LinkExtractor(allow=r&#x27;.+0!3!0!0!0!!!!1!1!\\d+&#x27;),follow=True), Rule(LinkExtractor(allow=r&#x27;.+work/.+html&#x27;),follow=False,callback=&quot;parse_detail&quot;) ) def parse_detail(self, response): image_urls = response.xpath(&quot;//div[@class=&#x27;photo-information-content&#x27;]/img/@src&quot;).getall() title = response.xpath(&quot;//div[@class=&#x27;details-contitle-box&#x27;]/h2/text()&quot;).getall() title = &quot;&quot;.join(title).strip() item = ZcoolItem(title=title,image_urls=image_urls) return item items.py 12345import scrapyclass ZcoolItem(scrapy.Item): title = scrapy.Field() image_urls = scrapy.Field() images = scrapy.Field() piplines.py 123456789101112131415161718from scrapy.pipelines.images import ImagesPipelinefrom zcool import settingsimport osimport reclass ZcoolPipeline(ImagesPipeline): def get_media_requests(self, item, info): media_requests = super(ZcoolPipeline, self).get_media_requests(item,info) for media_request in media_requests: media_request.item = item return media_requests def file_path(self, request, response=None, info=None, *, item=None): origin_path = super(ZcoolPipeline, self).file_path(request, response, info) # 先执行一遍原函数 title = request.item[&#x27;title&#x27;] title = re.sub(r&#x27;[\\\\/:\\*\\?&quot;&lt;&gt;\\|]&#x27;,&quot;&quot;,title) save_path = os.path.join(settings.IMAGES_STORE,title) image_name = origin_path.replace(&quot;full/&quot;,&quot;&quot;) return os.path.join(save_path,image_name) settings.py 123456789101112131415161718192021222324252627BOT_NAME = &#x27;zcool&#x27;SPIDER_MODULES = [&#x27;zcool.spiders&#x27;]NEWSPIDER_MODULE = &#x27;zcool.spiders&#x27;# Obey robots.txt rulesROBOTSTXT_OBEY = False# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;, &#x27;Accept-Language&#x27;: &#x27;en&#x27;, &#x27;User-Agent&#x27; : &#x27;我的user-agent&#x27;&#125;# Configure item pipelines# See https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; &#x27;zcool.pipelines.ZcoolPipeline&#x27;: 300,&#125;import osIMAGES_STORE = os.path.join(os.path.dirname(os.path.dirname(__file__)),&#x27;images&#x27;)","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/"},{"name":"实战","slug":"爬虫/实战","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"scrapy","slug":"scrapy","permalink":"https://pyxblog.cn/tags/scrapy/"}]},{"title":"scrapy爬虫实战02-CrawlSpider入门","slug":"python-spider-scrapy2","date":"2022-08-06T16:12:29.000Z","updated":"2022-08-06T16:16:02.052Z","comments":true,"path":"2022/08/07/python-spider-scrapy2/","link":"","permalink":"https://pyxblog.cn/2022/08/07/python-spider-scrapy2/","excerpt":"","text":"目标网站：传送门 CrawlSpider爬虫的创建 为什么要有CrawlSpider爬虫 spider是Scrapy框架中的基础爬虫，在翻页的时候，我们是这样操作的： 123456# 获取下一页next_href = response.xpath(\"//a[@id='amore']/@href\").get()if next_href: next_url = response.urljoin(next_href) request = scrapy.Request(next_url) yield request 而比spider高级一点的CrawlSpider爬虫，其主要特色是不用手动yield，可以实现遇到指定URL后自动翻页，这就比spider方便一些。 创建CrawlSpider爬虫的命令： 1scrapy genspider -t crawl [爬虫名字] [域名] 参考上面的创建流程，我们在终端中输入下面四行代码： 12345cd /Users/pangyuxuan/lyCrawlSpider # cd到文件夹lyCrawlSpiderscrapy startproject lycs # 创建Scrapy项目，项目名称为lycscd lycs # 进入项目路径scrapy genspider -t crawl lycSpider https://www.lieyunwang.com/ # 创建crawl爬虫，爬虫名称为lycSpider，目标域名为https://www.lieyunwang.com/ 得到了这样的爬虫文件： 与spider的区别——“规则”的定义 spiders文件夹中的lycSpider.py与基础案例中的gsw_spider.py相对应，其默认的代码如下： 123456789101112131415161718192021import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleclass LycspiderSpider(CrawlSpider): name = 'lycSpider' # 没变 allowed_domains = ['https://www.lieyunwang.com/'] # 没变 start_urls = ['http://https://www.lieyunwang.com//'] # 没变 rules = ( # 满足rules时自动爬取，不用再手动yield Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) def parse_item(self, response): item = {} #item['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').get() #item['name'] = response.xpath('//div[@id=\"name\"]').get() #item['description'] = response.xpath('//div[@id=\"description\"]').get() return item 其中： LinkExtractor 使用LinkExtractor可以在页面中自动找到所有满足规则的url，实现自动的爬取。 1class scrapy.linkextractors.LinkExtractor(allow = (),deny = (),allow_domains = (),deny_domains = (),deny_extensions = None,restrict_xpaths = (),tags = ('a','area'),attrs = ('href'),canonicalize = True,unique = True,process_value = None) 常用参数： allow ：允许的url——所有满足这个正则表达式的url都会被提取。 deny ：禁止的url——所有满足这个正则表达式的url都不会被提取。 allow_domains ：允许的域名——只有在这个里面指定的域名的url才会被提取。 deny_domains ：禁止的域名——所有在这个里面指定的域名的url都不会被提取。 restrict_xpaths ：使用xpath——和allow共同过滤链接。 Rule 用来定义这个url爬取后的处理方式，比如是否需要跟进，是否需要执行回调函数等。 1class scrapy.spiders.Rule(link_extractor, callback = None, cb_kwargs = None, follow = None,process_links = None, process_request = None) 常用参数： link_extractor ：一个LinkExtractor对象，用于定义爬取规则 callback ：满足这个规则的url，应该要执行哪个回调函数。 follow ：指定根据该规则从response中提取的链接是否需要跟进，也就是需不需要找这个链接的页面里还有没有其他符合要求的链接 process_links ：从link_extractor中获取到链接后会传递给这个函数，用来过滤不需要爬取的链接 实操 先在settings.py里关闭协议、设置ua 123456789# Obey robots.txt rulesROBOTSTXT_OBEY = False# Override the default request headers:DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent' : '我的user-'} 我们打开猎云网主页https://www.lieyunwang.com/ 调整start_urls 在页码1处点击检查，点击下方的链接进入第一页，复制此时浏览器内的链接即可： 1start_urls = ['https://www.lieyunwang.com/latest/p1.html'] 编写rules 我们的思路是： 找到每一页的链接，再从每一页里找每一篇文章的链接。 规则应该是这样： 1234rules = ( Rule(LinkExtractor(allow=r'/latest/p\\d+\\.html'), follow=True), Rule(LinkExtractor(allow=r'/archives/\\d+'), callback=\"parse_detail\", follow=False), ) 其中： 第一条规则用于找到每一页，因为页面的格式都是这样： 所以使用正则表达式匹配字符，即为/latest/p\\d+\\.html，其中： 页码可能是两位数，所以用d+ .是特殊符号，需要额外加一个反斜杠\\ 此外，找到每一页并不是终点，我们还需要找这一页里的文章，也就是还需要从这一页里面找其他链接，所以follow=True 第二条规则用于找每一页里的所有文章，因为文章的格式都是这样： 所以我们的正则表达式写为/archives/\\d+ 此外，我们找到文章以后，并不需要通过该文章找其他文章，所以follow=False，另外我们需要调用函数来获取它的内容了，所以callback=\"parse_detail\"，其中parse_detail是后面要写的函数 在parse_detail函数里测试一下我们的rules写没写对： 1234567891011121314151617181920import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleclass LycspiderSpider(CrawlSpider): name = 'lycSpider' allowed_domains = ['lieyunwang.com'] start_urls = ['https://www.lieyunwang.com/latest/p1.html'] rules = ( Rule(LinkExtractor(allow=r'/latest/p\\d+\\.html'), follow=True), Rule(LinkExtractor(allow=r'/archives/\\d+'), callback=\"parse_detail\", follow=False), ) def parse_detail(self, response): print(\"=\"*50) print(response.url) # 输出找到的url来验证 print(\"=\"*50) 哈哈对了！不对我会写在博客里吗 ps. 运行方法与前面的运行方法一致，都是新建一个start.py文件，可以先跳到后面去看一下start.py文件怎么写，也可以省略这一步测试（反正它一定是对的就是了，哼） 数据解析与存储 方便起见，我们只爬取文章的标题title、导语conclude和段落内容content 数据解析和存储的方式与之前的完全一样，在这里直接给出操作流程，不做过多的赘述： 1. 使用xpath获取三个部分的内容 直接右键-检查-copy xpath即可 1234567def parse_detail(self, response): title = response.xpath('//*[@id=\"fixed_container\"]/div[1]/div[2]/div[1]/h1/text()').getall() title = \"\".join(title).strip() content = response.xpath('//*[@id=\"main-text-id\"]').getall() content = \"\".join(content).strip() conclude = response.xpath('//*[@id=\"fixed_container\"]/div[1]/div[2]/div[3]').getall() conclude = \"\".join(conclude).strip() 2. 在settings.py中解除piplines的注释 12345# Configure item pipelines# See https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = { 'lycs.pipelines.LycsPipeline': 300,} 3. 编写piplines.py 12345678910111213from itemadapter import ItemAdapterimport jsonclass LycsPipeline: def open_spider(self,spider): self.fp = open(\"简讯.txt\",'w',encoding='utf-8') def process_item(self, item, spider): self.fp.write(json.dumps(dict(item),ensure_ascii=False)+'\\n') return item def close_spider(self,spider): self.fp.close() 4. 编写items.py 12345import scrapyclass LycsItem(scrapy.Item): title = scrapy.Field() # 标题 content = scrapy.Field() # 导语 conclude = scrapy.Field() # 结论 5. 在lycSPider.py里导入items并传入要保存的参数 1234567... # 省略其他部分代码from ..items import LycsItem... # 省略其他部分代码 def parse_detail(self, response): ... # 省略其他部分代码 item = LycsItem(title=title,content=content,conclude=conclude) return item # 写yield应该也可以 运行 CrawlSpider的运行与spider完全一样，都是在终端输入命令以运行，方便起见，我们还是编写start.py文件来实现在pycharm里的运行： 12from scrapy import cmdlinecmdline.execute(\"scrapy crawl lycSpider\".split(\" \")) ps. 为了秀一把这里给出了另一种发送命令的方式，本质上与之前那一种是一样的。 直接成功！ 总结 相比于spider，CrawlSpider的核心优势就是可以自己找新的页面，不用我们手动设置翻页方法。 最终的参考代码 lycSpider.py 123456789101112131415161718192021222324import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom ..items import LycsItemclass LycspiderSpider(CrawlSpider): name = 'lycSpider' allowed_domains = ['lieyunwang.com'] start_urls = ['https://www.lieyunwang.com/latest/p1.html'] rules = ( Rule(LinkExtractor(allow=r'/latest/p\\d+\\.html'), follow=True), Rule(LinkExtractor(allow=r'/archives/\\d+'), callback=\"parse_detail\", follow=False), ) def parse_detail(self, response): title = response.xpath('//*[@id=\"fixed_container\"]/div[1]/div[2]/div[1]/h1/text()').getall() title = \"\".join(title).strip() content = response.xpath('//*[@id=\"main-text-id\"]').getall() content = \"\".join(content).strip() conclude = response.xpath('//*[@id=\"fixed_container\"]/div[1]/div[2]/div[3]').getall() conclude = \"\".join(conclude).strip() item = LycsItem(title=title,content=content,conclude=conclude) return item items.py 12345import scrapyclass LycsItem(scrapy.Item): title = scrapy.Field() content = scrapy.Field() conclude = scrapy.Field() piplines.py 1234567891011from itemadapter import ItemAdapterimport jsonclass LycsPipeline: def open_spider(self,spider): self.fp = open(\"简讯.txt\",'w',encoding='utf-8') def process_item(self, item, spider): self.fp.write(json.dumps(dict(item),ensure_ascii=False)+'\\n') return item def close_spider(self,spider): self.fp.close() settings.py 123456789101112131415161718192021222324252627BOT_NAME = 'lycs'SPIDER_MODULES = ['lycs.spiders']NEWSPIDER_MODULE = 'lycs.spiders'# Obey robots.txt rulesROBOTSTXT_OBEY = False# Override the default request headers:DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent' : '我的user-agent'}# Configure item pipelines# See https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = { 'lycs.pipelines.LycsPipeline': 300,}# Override the default request headers:DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36'}","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/"},{"name":"实战","slug":"爬虫/实战","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"scrapy","slug":"scrapy","permalink":"https://pyxblog.cn/tags/scrapy/"}]},{"title":"Scrapy爬虫实战01-古诗文网","slug":"python-spider-scrapy1","date":"2022-08-06T16:06:53.000Z","updated":"2022-08-06T16:12:16.581Z","comments":true,"path":"2022/08/07/python-spider-scrapy1/","link":"","permalink":"https://pyxblog.cn/2022/08/07/python-spider-scrapy1/","excerpt":"","text":"ps. 案例制作时的操作环境是MacOS，如果是windows用户，下文中提到的“终端”指的就是cmd命令行窗口。 pps. 本文省略了安装过程，尚未安装scrapy的用户可以直接在pycharm的preference内搜索安装。 目标网站：传送门 任务：使用Scrapy框架爬虫，爬取“推荐”中共10页的古诗题目、作者、朝代和内容 ps. 各类教程都拿它举例子，古诗文网好惨 项目创建 创建Scrapy爬虫项目需要在终端中进行 先打开一个文件路径，即你希望的爬虫文件存放路径，比如我放在创建好的spidertest 文件夹中： 1cd /Users/pangyuxuan/spidertest # 这是文件夹路径 使用命令创建项目： 1scrapy startproject [项目名称] 创建爬虫： 12cd [项目名称] # 先进入项目路径scrapy genspider [爬虫名称] [目标域名] # 再创建爬虫文件 至此你已经创建好了scrapy爬虫文件，它应该长这样： 其中[项目名称]为gsw_test，[爬虫名称]为gsw_spider 综上，创建一个基本的scrapy爬虫文件，一共在终端的命令行中输入了4行代码： 12345cd /Users/pangyuxuan/spidertest # 打开一个文件路径，作为爬虫的存放路径scrapy startproject gsw_test # 创建scrapy项目，名为gsw_testcd gsw_test # 打开项目路径scrapy genspider gsw_spider https://www.gushiwen.org # 创建scrapy爬虫，爬虫名为gsw_spider，目标域名为 https://www.gushiwen.org 各个文件的作用 后续的编写还是依赖pycharm，所以在pycharm中打开项目文件： 其中各个文件的作用如下： settings.py：用来配置爬虫的，比如设置User-Agent、下载延时、ip代理。 middlewares.py：用来定义中间件。 items.py：用来提前定义好需要下载的数据字段。 pipelines.py：用来保存数据。 scrapy.cfg：用来配置项目。 爬取第一页的内容并保存 以下内容请按顺序阅读并实现 设置settings.py 先在settings.py中做两项工作： 设置robots.txt协议为“不遵守” robots.txt是一个互联网爬虫许可协议，默认是True（遵守协议），如果遵守的话大部分网站都无法进行爬取，所以先把这个协议的状态设为不遵守 12# Obey robots.txt rulesROBOTSTXT_OBEY = False ps. 所以这个协议的意义是什么。。。 配置请求头（设置user-agent） 123456# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;, &#x27;Accept-Language&#x27;: &#x27;en&#x27;, &#x27;user-agent&#x27; : &#x27;我自己的ua&#x27;&#125; 主要工作——编写gsw_spider.py 123456789import scrapyclass GswSpiderSpider(scrapy.Spider): # 我们的代码都写在这个类里面 name = &#x27;gsw_spider&#x27; # 爬虫的名字 allowed_domains = [&#x27;https://www.gushiwen.org&#x27;] # 目标域名 start_urls = [&#x27;http://https://www.gushiwen.org/&#x27;] # 爬虫的起始网页 def parse(self, response): 目前的爬虫起始网页start_urls是自动生成的，我们把它换成古诗文网的第一页 1start_urls = [&#x27;https://www.gushiwen.cn/default_1.aspx&#x27;] 为了使打印出来的结果更加直观，我们编写myprint函数如下： 1234def myprint(self,value): print(&quot;=&quot;*30) # 在输出内容的上、下加一些&#x27;=&#x27;，找起来方便 print(value) print(&quot;=&quot;*30) 然后我们尝试打印一下当前爬取到的内容，应该为古诗文网第一页的信息。 目前为止，gsw_spider.py被改成了这样： 12345678910111213141516import scrapyclass GswSpiderSpider(scrapy.Spider): name = &#x27;gsw_spider&#x27; # 爬虫的名字 allowed_domains = [&#x27;https://www.gushiwen.org&#x27;] # 目标域名 start_urls = [&#x27;https://www.gushiwen.cn/default_1.aspx&#x27;] # 起始页面 def myprint(self,value): print(&quot;=&quot;*30) print(value) print(&quot;=&quot;*30) def parse(self, response): self.myprint(response.text) # 打印网页源代码 运行的方法 scrapy爬虫需要在终端里输入命令来运行，输入命令如下： 1scrapy crawl gsw_spider # gsw_spider是爬虫名 方便起见，我们在项目目录里新建一个start.py，通过cmdline库里的函数来向终端发送命令，这样就不用不停地切换窗口了，而且运行结果可以在pycharm里直接展现，这样就与我们之前学的爬虫一样了。 后续我们无论修改哪个代码，都是运行start.py这个文件。 123from scrapy import cmdlinecmds = [&#x27;scrapy&#x27;,&#x27;crawl&#x27;,&#x27;gsw_spider&#x27;] # 拼接命令语句cmdline.execute(cmds) # 执行 点击运行，可以在运行窗口中看到结果： 截至目前为止，我们已经获取了网页源代码，接下来的工作就是从源代码中解析想要的数据了。 无需导入新的库，Scrapy框架为我们内置了许多函数，使我们仍可以用之前学习的数据解析知识（xpath、bs4和正则表达式）来完成数据提取。 使用xpath提取数据 使用xpath语法提取数据，返回的结果是选择器列表类型SelectorList，选择器列表里包含很多选择器Selector，即： response.xpath返回的是SelectorList对象 SelectorList存储的是Selector对象 我们获取一下所有包含古诗标题的标签，输出返回值类型，以验证上面的结论： 12345def parse(self, response): gsw_divs = response.xpath(&quot;//div[@class=&#x27;left&#x27;]/div[@class=&#x27;sons&#x27;]&quot;) self.myprint(type(gsw_divs)) # 打印获取到的div标签集的类型 for gsw_div in gsw_divs : self.myprint(type(gsw_div)) # 打印标签集中的每个元素的类型 运行结果： 使用get()或getall()函数从选择器类型的数据中提取需要的数据： get()返回选择器的第一个值（字符串类型） getall()返回选择器的所有值（列表类型） 12345for gsw_div in gsw_divs : title_get = gsw_div.xpath(&quot;.//b/text()&quot;).get() title_getall = gsw_div.xpath(&quot;.//b/text()&quot;).getall() self.myprint(title_get) # 打印get函数的结果 self.myprint(title_getall) # 打印getall函数的结果 输出： 我们共提取标题、朝代、作者、内容四部分信息，gsw_spider.py代码如下： 1234567891011121314151617181920212223242526import scrapyclass GswSpiderSpider(scrapy.Spider): name = &#x27;gsw_spider&#x27; # 爬虫的名字 allowed_domains = [&#x27;https://www.gushiwen.org&#x27;] # 目标域名 start_urls = [&#x27;https://www.gushiwen.cn/default_1.aspx&#x27;] # 起始页面 def myprint(self,value): # 用于打印的函数 print(&quot;=&quot;*30) print(value) print(&quot;=&quot;*30) def parse(self, response): gsw_divs = response.xpath(&quot;//div[@class=&#x27;left&#x27;]/div[@class=&#x27;sons&#x27;]&quot;) for gsw_div in gsw_divs : title = gsw_div.xpath(&quot;.//b/text()&quot;).get() # 题目 source = gsw_div.xpath(&quot;.//p[@class=&#x27;source&#x27;]/a/text()&quot;).getall() # 朝代+作者 # source是getall函数的返回值，是个列表，故可以直接用下标调用 dynasty = source[0] # 朝代 writer = source[1] # 作者 self.myprint(source) content = gsw_div.xpath(&quot;.//div[@class=&#x27;contson&#x27;]//text()&quot;).getall() # 诗文内容 # 用//text()获取标签下的所有文本 content = &#x27;&#x27;.join(content).strip() # 将列表拼接,并用strip()删除前后的换行/空格 你可以在任意地方插入self.myprint(内容)来进行打印，以验证数据是否被成功提取 接下来就是保存数据，我们先在items.py中配置好要保存的数据有哪些。 配置items.py 还记得这个文件是干什么用的吗？ items.py：用来提前定义好需要下载的数据字段。 一共有上述四部分内容需要保存，因此我们的items.py应该这样写： 1234567import scrapyclass GswTestItem(scrapy.Item): title = scrapy.Field() # 标题 dynasty = scrapy.Field() # 朝代 writer = scrapy.Field() # 作者 content = scrapy.Field() # 内容 其中Field()可以理解为一种普适的变量类型，不管是字符串还是列表，都用scrapy.Field()来接收。 在gsw_spider.py里导入items 定义完items.py后，我们在gsw_spiders.py里导入它。需要注意的是，gsw_spiders.py在spiders文件夹里，也就是说items.py在gsw_spiders.py的上层目录中： 因此导入时，应该这样写： 1from ..items import GswTestItem # ..表示上层目录 导入后，我们将对应参数传入，然后使用yield关键字进行返回 12item = GswTestItem(title=title,dynasty=dynasty,writer=writer,content=content)yield item 进入pipelines.py和settings.py 先在settings.py里把pipelines.py打开： 123456# Configure item pipelines# See https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; &#x27;gsw_test.pipelines.GswTestPipeline&#x27;: 300, # 300是这个pipeline的优先级，代表了执行顺序，数值越小优先级越大&#125; 再编写pipelines.py 123456789101112131415161718from itemadapter import ItemAdapterimport json # 记得自己导入json库class GswTestPipeline: def open_spider(self,spider): self.fp = open(&quot;古诗文.txt&quot;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) # 制定文件名和编码格式 def process_item(self, item, spider): self.fp.write(json.dumps(dict(item),ensure_ascii=False)+&#x27;\\n&#x27;) # dict函数将item转化为字典 # json.dumps()将字典格式的item转换为json字段 # 参数ensure_ascii=False,用于存储中文 # +&#x27;\\n&#x27;用于将保存的内容自动换行 return item def close_spider(self,spider): # 关闭文件 self.fp.close() 上面的open_spider函数和close_spider函数虽然不是自带的，但它是一种模版化的函数（套路），是一种Scrapy框架提供的高效的文件存储形式。 我们自己写的时候，只要按上述样式编（默）写即可，根据自己的需求修改存储文件的文件名、格式和编码方式，但不能改变两个函数名！ 现在我们运行start.py，就会发现路径下多了一个古诗文.txt，打开以后是这样： 至此，第一页爬取成功！（不要在意为什么只爬了一点就结束了，先往下看，最后会有修正） 爬取后续内容 爬取了第一页的内容以后，我们还需要继续往后寻找，先来找一下第二页的url： 右键检查“下一页”按钮以获取下一页的url 为了测试寻找下一页的功能，我们暂时忽略之前的代码 1234def parse(self, response): next_href = response.xpath(&quot;//a[@id=&#x27;amore&#x27;]/@href&quot;).get() # 获取href属性 next_url = response.urljoin(next_href) # 给/default_2.aspx添加前缀域名使其变完整 self.myprint(next_url) # 输出以验证 找到了！ 接下来我们就用一个request来接收scrapy.Request(next_url)的返回值，并使用yield关键字来返回即可： 1234next_href = response.xpath(&quot;//a[@id=&#x27;amore&#x27;]/@href&quot;).get()next_url = response.urljoin(next_href)request = scrapy.Request(next_url)yield request 需要注意的是，我们需要给“寻找下一页”操作设立一个终止条件，当下一页不存在的时候停止访问，所以最后的代码长这个样子： 123456# 获取下一页next_href = response.xpath(&quot;//a[@id=&#x27;amore&#x27;]/@href&quot;).get()if next_href: next_url = response.urljoin(next_href) request = scrapy.Request(next_url) yield request 针对反爬虫机制的修改完善 此时我们的代码是这样的： gsw_spider.py 123456789101112131415161718192021222324252627282930313233import scrapyfrom ..items import GswTestItemclass GswSpiderSpider(scrapy.Spider): name = &#x27;gsw_spider&#x27; # 爬虫的名字 allowed_domains = [&#x27;https://www.gushiwen.org&#x27;] # 目标域名 start_urls = [&#x27;https://www.gushiwen.cn/default_1.aspx&#x27;] def myprint(self,value): print(&quot;=&quot;*30) print(value) print(&quot;=&quot;*30) def parse(self, response): gsw_divs = response.xpath(&quot;//div[@class=&#x27;left&#x27;]/div[@class=&#x27;sons&#x27;]&quot;) for gsw_div in gsw_divs : title = gsw_div.xpath(&quot;.//b/text()&quot;).get() # 古诗题目 source = gsw_div.xpath(&quot;.//p[@class=&#x27;source&#x27;]/a/text()&quot;).getall() # 朝代+作者 # source是getall函数的返回值，是个列表，直接用下标调用 dynasty = source[0] # 朝代 writer = source[1] # 作者 content = gsw_div.xpath(&quot;.//div[@class=&#x27;contson&#x27;]//text()&quot;).getall() # 诗文内容 # 用//text()获取标签下的所有文本 content = &#x27;&#x27;.join(content).strip() # 将列表拼接,并用strip()删除前后的换行/空格 item = GswTestItem(title=title,dynasty=dynasty,writer=writer,content=content) yield item # 获取下一页 next_href = response.xpath(&quot;//a[@id=&#x27;amore&#x27;]/@href&quot;).get() if next_href: next_url = response.urljoin(next_href) request = scrapy.Request(next_url) yield request 运行后，会报这样一个错误：IndexError: list index out of range，意思是“列表的下标索引超过最大区间”。 为什么会有这样的错误呢？ 我们可以在网页上看到，页面上不全是古诗文： 除了古诗文外，这种短句子也是在class=sons的标签下，按照我们的查找方式： 123gsw_divs = response.xpath(&quot;//div[@class=&#x27;left&#x27;]/div[@class=&#x27;sons&#x27;]&quot;)for gsw_div in gsw_divs : source = gsw_div.xpath(&quot;.//p[@class=&#x27;source&#x27;]/a/text()&quot;).getall() 找到图中蓝色的div标签以后，它里面是没有p标签的，也就是说此时的source是个空表，直接调用source[0]那必然是要报错的。 这算是网站的一种反爬虫机制，利用格式不完全相同的网页结构来让你的爬虫报错，太狠了！！ 为了解决这个问题，我们添加try...except结构如下： 123456789101112for gsw_div in gsw_divs : title = gsw_div.xpath(&quot;.//b/text()&quot;).get() source = gsw_div.xpath(&quot;.//p[@class=&#x27;source&#x27;]/a/text()&quot;).getall() try: dynasty = source[0] writer = source[1] content = gsw_div.xpath(&quot;.//div[@class=&#x27;contson&#x27;]//text()&quot;).getall() content = &#x27;&#x27;.join(content).strip() item = GswTestItem(title=title,dynasty=dynasty,writer=writer,content=content) yield item except: print(title) # 打印出错的标题以备检查 这样，上面的报错就被完美解决了。 然鹅，一波未平一波又起，bug永远是生生不息源源不绝的 我们发现了一个新的报错：DEBUG: Filtered offsite request to 'www.gushiwen.cn': &lt;GET https://www.gushiwen.cn/default_2.aspx&gt; 这是因为我们在最开始的allowed_domains里限制了访问的域名：\"https://www.gushiwen.org\" 而到了第二页的时候，网站偷偷把域名换成.cn了！ .cn不是.org，我们的爬虫没法继续访问，所以就停了。这又是这个网站的一个反爬虫机制，我们只需要在allowed_domains里添加一个.cn的域名，这个问题就可以得到妥善的解决： 1allowed_domains = [&#x27;gushiwen.org&#x27;,&#x27;gushiwen.cn&#x27;] 运行可得到期望结果： 最终的古诗文网Scrapy爬虫代码 gsw_spider.py 12345678910111213141516171819202122232425262728293031323334353637import scrapyfrom ..items import GswTestItemclass GswSpiderSpider(scrapy.Spider): name = &#x27;gsw_spider&#x27; # 爬虫的名字 # allowed_domains = [&#x27;https://www.gushiwen.org&#x27;] # 目标域名 allowed_domains = [&#x27;gushiwen.org&#x27;,&#x27;gushiwen.cn&#x27;] start_urls = [&#x27;https://www.gushiwen.cn/default_1.aspx&#x27;] def myprint(self,value): print(&quot;=&quot;*30) print(value) print(&quot;=&quot;*30) def parse(self, response): gsw_divs = response.xpath(&quot;//div[@class=&#x27;left&#x27;]/div[@class=&#x27;sons&#x27;]&quot;) for gsw_div in gsw_divs : title = gsw_div.xpath(&quot;.//b/text()&quot;).get() # 古诗题目 source = gsw_div.xpath(&quot;.//p[@class=&#x27;source&#x27;]/a/text()&quot;).getall() # 朝代+作者 # source是getall函数的返回值，是个列表，直接用下标调用 try: dynasty = source[0] # 朝代 writer = source[1] # 作者 content = gsw_div.xpath(&quot;.//div[@class=&#x27;contson&#x27;]//text()&quot;).getall() # 诗文内容 # 用//text()获取标签下的所有文本 content = &#x27;&#x27;.join(content).strip() # 将列表拼接,并用strip()删除前后的换行/空格 item = GswTestItem(title=title,dynasty=dynasty,writer=writer,content=content) yield item except: print(title) # 获取下一页 next_href = response.xpath(&quot;//a[@id=&#x27;amore&#x27;]/@href&quot;).get() if next_href: next_url = response.urljoin(next_href) request = scrapy.Request(next_url) yield request items.py 123456789101112131415# Define here the models for your scraped items## See documentation in:# https://docs.scrapy.org/en/latest/topics/items.htmlimport scrapyclass GswTestItem(scrapy.Item): # define the fields for your item here like: title = scrapy.Field() dynasty = scrapy.Field() writer = scrapy.Field() content = scrapy.Field() pipelines.py 12345678910111213from itemadapter import ItemAdapterimport jsonclass GswTestPipeline: def open_spider(self,spider): self.fp = open(&quot;古诗文.txt&quot;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) def process_item(self, item, spider): self.fp.write(json.dumps(dict(item),ensure_ascii=False)+&#x27;\\n&#x27;) # dict函数将item转化为字典,再转换为json字段进行保存 return item def close_spider(self,spider): self.fp.close() settings.py 为了看起来简洁一点，注释部分我就都删了 123456789101112131415161718192021BOT_NAME = &#x27;gsw_test&#x27;SPIDER_MODULES = [&#x27;gsw_test.spiders&#x27;]NEWSPIDER_MODULE = &#x27;gsw_test.spiders&#x27;# Obey robots.txt rulesROBOTSTXT_OBEY = False# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;, &#x27;Accept-Language&#x27;: &#x27;en&#x27;, &#x27;user-agent&#x27; : &#x27;我的user-agent&#x27;&#125;# Configure item pipelines# See https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; &#x27;gsw_test.pipelines.GswTestPipeline&#x27;: 300,&#125; 大功告成！自己试一下吧！","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/"},{"name":"实战","slug":"爬虫/实战","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"scrapy","slug":"scrapy","permalink":"https://pyxblog.cn/tags/scrapy/"}]},{"title":"python爬虫实战演示","slug":"python-spider-demo1","date":"2022-08-06T15:56:53.000Z","updated":"2022-08-06T16:06:17.936Z","comments":true,"path":"2022/08/06/python-spider-demo1/","link":"","permalink":"https://pyxblog.cn/2022/08/06/python-spider-demo1/","excerpt":"","text":"猫眼专业版实时票房数据获取 网址：http://piaofang.maoyan.com/dashboard 错误方法1： 1234import requestsurl = &#x27;http://piaofang.maoyan.com/dashboard&#x27;resp = requests.get(url)print(resp.content.decode(&#x27;utf-8&#x27;)) 点开“检查网页源代码”，发现输出和源代码不一样 问题出在请求头上，连User-agent都没有，稍微走点心的网站都知道你是爬虫了 Ps.user-agent是什么：user-agent会告诉网站，访问者是通过什么工具来请求的，如果是爬虫请求，一般会拒绝；如果是用户浏览器，就会应答。我们在浏览器里获取的user-agent添加到爬虫中，网站检测这项数据时会把它当成你自己用的那个浏览器，以起到瞒天过海的作用。 错误方法2： 1234567import requestsurl = &#x27;http://piaofang.maoyan.com/dashboard&#x27;headers = &#123; # 添加一个请求头 &#x27;User-Agent&#x27; : &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&#x27;&#125;resp = requests.get(url,headers=headers)print(resp.content.decode(&#x27;utf-8&#x27;)) 这下输出和网页源代码一样了，兴高采烈找数据，发现没有！ 原来是因为猫眼专业版的数据是实时更新的，因此它没有保存在静态的网页结构里，而是通过json文件实时发送，所以我们在网页源代码中是找不到数据的，数据在哪呢？ 点开“检查”，在Network里你会发现，网页在不停的请求文件，选中右边的Response，仔细一找，数据就在里面！ 当然，如果你觉得麻烦，这里推荐一个格式解析工具，可以自动把json文件里的代码以更美观的角度呈现。 在线的json解析工具：https://www.json.cn/ 所以，我们请求静态网页的链接，是找不到数据的，而把链接换成发送数据包的链接，就可以得到数据了。 正确方法： 12345678910import requests# 使用json文件里的urlurl = &#x27;http://piaofang.maoyan.com/dashboard-ajax?orderType=0&amp;uuid=176e6479baec8-0cc8072b9a3fd4-171d4b58-13c680-176e6479bafc8&amp;riskLevel=71&amp;optimusCode=10&amp;_token=eJxNkctqw0AMRf9l1sKRRvOyIYtAoaTQRUPaTchi8qgTSuLgmNJS%2Bu%2BVJi4tGO7x1cOS%2FGX6%2Bc40COZ935vGUIVVMGCGq2koEHmk6DlhALP98yySxTqC2fQvd6ZZEXECcdbqLMRYkXMINeIa%2FiGjPJozlxRzGIZLM5lcjrl7zee2OuXuM5%2BrbXea7PL1sOlyv5NJjBScllrAkQHF4RiBVJP0K8rgiyZIqjVDXTQBWQGHDigUiCNQDVZ7OGtHYAZbQhxGcBG4hLwF5gI1cAkFcaJCRHDlEzrDDTz4kixTeB3DI0PkAgGiVnkrB7mBBULdyTMBldG8XIvYKXnxSi8fZIGou%2Fmoh056lDc9imgedfh9f5T%2FKKnXY3sW2j98LJ%2Fb%2BWx2384WT9Op%2Bf4B3HBuKQ%3D%3D&#x27;headers = &#123; &#x27;User-Agent&#x27; : &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&#x27;&#125;resp = requests.get(url,headers=headers)print(resp.content.decode(&#x27;utf-8&#x27;)) 一些其他的动态网页，可能并不是实时更新，但也是采用同样的数据显示方式，这就需要你仔细的找一下数据到底藏在哪个文件里，点开Response后从上往下捋就行了。 当然你也可以使用selenium方法去模拟浏览器的行为，在这里就不细说了 石头阅读模拟登陆 石头阅读是一款免费的小说阅读器，书库覆盖面广，资源丰富，可以免费看各类需要付费的小说，堪称白嫖党的利器，趁此机会安利一下 网址：https://www.stoneread.com/ 登陆在右上角，有点隐蔽： 模拟登陆大体上有两种，即添加cookie和使用post请求添加账号密码。由于后面的实战会涉及到前一种，所以这个实战就先用后一种了。 核心思路是，获取网站用以验证账号密码的url，然后把我们准备好的账号密码以post请求的形式发给它，这样就相当于登陆成功了。 怎么找目标url？ 我们先手动登陆试试，点击登录后，注视着“检查”里的Network栏，你会发现一个“一闪即逝”的文件： \"小老弟，跑的挺快啊\" 我们点击左上角的停止键，就可以获得这个文件，点开就有了url，顺便再抄一下user-agent 翻到下面，有一个“Form Data”，一看就是我们提交的账号密码： 不许盗号！！！ 这个checkbox代表的是那个“下次自动登录”选项，on自然就是表示“勾选” 代码： 1234567891011121314import requestsurl = &#x27;https://www.stoneread.com/login/logincheck?ur=&#x27;headers = &#123; &#x27;User-Agent&#x27; : &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&#x27;&#125;data = &#123; &#x27;username&#x27; : &#x27;kaitoukiddo@yeah.net&#x27;, &#x27;password&#x27; : &#x27;12345ssdlh&#x27;, &#x27;checkbox&#x27; : &#x27;on&#x27;&#125;resp = requests.post(url,headers=headers,data=data)print(resp.content.decode(&#x27;utf-8&#x27;)) 结果中有“登陆成功”字样，和我们看到的页面一致。嘿嘿，成功啦～ 设置代理ip 这个方法它成功几率不太大，因为我用的是免费的代理ip，没氪金怎么会变强呢它就不太稳定。我先把验证方法给一下。 获取免费ip的网址： * 快代理：https://www.kuaidaili.com/free/ * 芝麻代理：http://http.zhimaruanjian.com/ * 太阳代理：http://http.taiyangruanjian.com/ * 讯代理：http://www.xdaili.cn/ * 蚂蚁代理：http://www.mayidaili.com/ * 极光代理：http://www.jiguangdaili.com/ 查看当前ip的请求url：http://www.httpbin.org/ip ps. http://www.httpbin.org/是一个功能很强大的网站，大家可以访问一下康康 我们先不用代理，看一下自己的ip地址 1234import requestsurl = &#x27;http://www.httpbin.org/ip&#x27;resp = requests.get(url)print(resp.text) 这个时候，就会打印出我自己电脑的真实ip 然后我们为它添加代理： 1234567import requestsproxy = &#123; # 免费的代理ip，这会儿估计已经没法用了 &#x27;http&#x27;:&#x27;111.77.197.127:9999&#x27;&#125;url = &#x27;http://www.httpbin.org/ip&#x27;resp = requests.get(url,proxies=proxy)print(resp.text) 这个时候如果你欧了一把，选的代理ip恰好是可以用的，那你就会看到程序打印出了你选的ip地址。 附： urllib库设置代理的方法——ProxyHandler处理器 12345678910111213from urllib import requesturl = &#x27;http://httpbin.org/ip&#x27;#1. 使用ProxyHandler,传入代理构建一个handler,代理的结构是字典handler = request.ProxyHandler(&#123;&#x27;http&#x27;:&#x27;122.193.244.243:9999&#x27;&#125;)#2. 使用上面创建的handler构建一个openeropener = request.build_opener(handler)#3. 使用opener去发送一个请求resp = opener.open(url)print(resp.read()) selenium设置代理的方法 1234options = webdriver.ChromeOptions() # 创建ChromeOptions对象options.add_argument(&quot;--proxy-server=htto://175.43.151.209:9999&quot;) # 代理driver = webdriver.Chrome(executable_path=&quot;/Users/pangyuxuan/Desktop/chromedriver&quot;,chrome_options=options) # 在driver路径后添加代理参数driver.get(&quot;http://httpbin.org/ip&quot;) 在Scrapy中设置代理 设置普通代理 12345678class IPProxyDownloadMiddleware(object): PROXIES = [ &quot;5.196.189.50:8080&quot;, ] def process_request(self,request,spider): proxy = random.choice(self.PROXIES) print(&#x27;被选中的代理：%s&#x27; % proxy) request.meta[&#x27;proxy&#x27;] = &quot;http://&quot; + proxy 设置独享代理 12345678class IPProxyDownloadMiddleware(object): def process_request(self,request,spider): proxy = &#x27;121.199.6.124:16816&#x27; user_password = &quot;970138074:rcdj35xx&quot; request.meta[&#x27;proxy&#x27;] = proxy # bytes b64_user_password = base64.b64encode(user_password.encode(&#x27;utf-8&#x27;)) request.headers[&#x27;Proxy-Authorization&#x27;] = &#x27;Basic &#x27; + b64_user_password.decode(&#x27;utf-8&#x27;) 爬取瓜子二手车交易信息 网址：https://www.guazi.com/www/buy/ 我们先什么都不加，直接请求网址，看输出是什么，再逐渐的添加请求头里的内容，来尝试网站究竟是用什么作为反爬虫的检测依据的。 什么都不加： 12345import requestsfrom lxml import etreeurl = &#x27;https://www.guazi.com/www/buy/&#x27;resp = requests.get(url)print(resp.text) 输出： 经过比较，它和网页源代码不一致，且最后还出现了乱码现象，说明text把解码方法猜错了——体现了content.decode('utf-8')的稳定性 添加User-agent： 12345678import requestsfrom lxml import etreeurl = &#x27;https://www.guazi.com/www/buy/&#x27;headers = &#123; &#x27;User-Agent&#x27; : &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&#x27;&#125;resp = requests.get(url,headers=headers)print(resp.content.decode(&#x27;utf-8&#x27;)) 哈哈！还是不行。 添加cookie： 123456789import requestsfrom lxml import etreeurl = &#x27;https://www.guazi.com/www/buy/&#x27;headers = &#123; &#x27;Cookie&#x27; : &#x27;uuid=8375fbff-6a87-4130-8c96-7a4aa8d30728; ganji_uuid=8329982395237678242215; cainfo=%7B%22ca_a%22%3A%22-%22%2C%22ca_b%22%3A%22-%22%2C%22ca_s%22%3A%22self%22%2C%22ca_n%22%3A%22self%22%2C%22ca_medium%22%3A%22-%22%2C%22ca_term%22%3A%22-%22%2C%22ca_content%22%3A%22-%22%2C%22ca_campaign%22%3A%22-%22%2C%22ca_kw%22%3A%22-%22%2C%22ca_i%22%3A%22-%22%2C%22scode%22%3A%22-%22%2C%22keyword%22%3A%22-%22%2C%22ca_keywordid%22%3A%22-%22%2C%22display_finance_flag%22%3A%22-%22%2C%22platform%22%3A%221%22%2C%22version%22%3A1%2C%22client_ab%22%3A%22-%22%2C%22guid%22%3A%228375fbff-6a87-4130-8c96-7a4aa8d30728%22%2C%22ca_city%22%3A%22qd%22%2C%22sessionid%22%3A%22dbe4532a-24f9-45fc-8c5f-b9518a2efb46%22%7D; antipas=93901707482fmWd51E58673WzOJ; cityDomain=www; clueSourceCode=%2A%2300; user_city_id=-1; preTime=%7B%22last%22%3A1611505261%2C%22this%22%3A1610175479%2C%22pre%22%3A1610175479%7D; sessionid=30370a7d-e999-43e4-a614-4ffacc7c8e75; lg=1; Hm_lvt_bf3ee5b290ce731c7a4ce7a617256354=1610175480,1610175573,1610524515,1611505262; Hm_lpvt_bf3ee5b290ce731c7a4ce7a617256354=1611505262&#x27;, &#x27;User-Agent&#x27; : &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&#x27;&#125;resp = requests.get(url,headers=headers)print(resp.content.decode(&#x27;utf-8&#x27;)) 哈哈！竟然行了！ 虽然我自己试的时候还额外添加了Host，但不知道为什么现在只加一个cookie就行了。 后续的思路就是我们通过“检查”，在Elements里找到详情页的链接如图： 并且不同的车存放在不同的&lt;li&gt;标签中： 所以我们遍历所有&lt;li&gt;标签，依次访问每辆车的详情页面，再获取详情页面的车辆信息，进行存储即可。感兴趣的同学可以自行完成后面的代码，也可以找我要一下之前的代码 爬取豆瓣top250 网址：https://movie.douban.com/top250 这个案例也是我学习时候，花比较多时间做的一个案例，虽然它没有什么很惊艳的实现技巧，但我在做的时候遇到了这样一个问题：我被封号了！ 考虑到我囊中羞涩，没有租60r/月的代理服务器，而免费代理它又不太稳定，于是我选择老老实实的注册登陆。没错我就是前几天才注册豆瓣 很简单，只要登录以后添加cookie信息就可以了。 代码如下，做到存储数据之前： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import requestsfrom bs4 import BeautifulSoupheaders = &#123; &#x27;User-Agent&#x27; : &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#x27;, &#x27;Cookie&#x27; : &#x27;ll=&quot;118221&quot;; bid=Dp60PRKNGWI; __yadk_uid=t4cy7TbKsr834lpYtjbt0Vn6au2yF7oP; _vwo_uuid_v2=D181BE4292803A62776208FF476CE0C21|403fd897a559fbd412c3a7530efb5d2f; __gads=ID=546d1adef9db7e77-221ec2b1c5c5000a:T=1611226743:RT=1611226743:S=ALNI_MZmgV9oiEJeTVAfAnDiNlwsRm8WMQ; ap_v=0,6.0; __utmc=30149280; __utmz=30149280.1611299250.2.2.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; __utmc=223695111; __utmz=223695111.1611299250.2.2.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1611302352%2C%22https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3DStwtEMgge76IHS2f9aoITF2La9KliUfVdz-ShjuHwB78oJQFmijIAnaCAMgiQfxo%26wd%3D%26eqid%3Dd8a7eb3b0003fac200000002600a79b0%22%5D; _pk_ses.100001.4cf6=*; __utma=30149280.233705150.1611225868.1611299250.1611302352.3; __utma=223695111.583107523.1611225868.1611299250.1611302352.3; __utmb=223695111.0.10.1611302352; dbcl2=&quot;231087110:rCDcDbFTf0I&quot;; ck=rEkU; push_noty_num=0; push_doumail_num=0; __utmt=1; __utmv=30149280.23108; __utmb=30149280.2.10.1611302352; _pk_id.100001.4cf6=fcbcfa39023ea437.1611225868.3.1611304162.1611299286.&#x27;&#125;# 函数：获取详情页面的url，返回一个列表def get_detail_urls(url): resp = requests.get(url, headers=headers) # 获取详情页面url html = resp.text soup = BeautifulSoup(html, &#x27;lxml&#x27;) lis = soup.find(&#x27;ol&#x27;, class_=&#x27;grid_view&#x27;).find_all(&#x27;li&#x27;) # 经搜索，grid_view属性值唯一，故用find直接找到即可 detail_urls = [] # 列表 for li in lis: detail_url = li.find(&#x27;a&#x27;)[&#x27;href&#x27;] detail_urls.append(detail_url) return detail_urlsdef parse_detail_url(url): resp = requests.get(url,headers=headers) html = resp.text soup = BeautifulSoup(html,&#x27;lxml&#x27;) lis = [] name = list(soup.find(&#x27;span&#x27;,property=&#x27;v:itemreviewed&#x27;).stripped_strings) name = &#x27;&#x27;.join(name) lis.append(name) director = list(soup.find(&#x27;div&#x27;,id=&#x27;info&#x27;).find_all(&#x27;span&#x27;)[0].stripped_strings) director = &#x27;&#x27;.join(director) lis.append(director) actor = list(soup.find(&#x27;div&#x27;,id=&#x27;info&#x27;).find(&#x27;span&#x27;,class_=&#x27;actor&#x27;).stripped_strings) actor = &#x27;&#x27;.join(actor) lis.append(actor) score = soup.find(&#x27;strong&#x27;,class_=&#x27;ll rating_num&#x27;).string lis.append(score) remark = list(soup.find(&#x27;span&#x27;,property=&#x27;v:summary&#x27;).stripped_strings) remark = &#x27;&#x27;.join(remark) lis.append(remark) print(lis)def main(): base_url = &#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27; for x in range(0,251,25): url = base_url.format(x) detail_urls = get_detail_urls(url) for detail_url in detail_urls: parse_detail_url(detail_url) # resp = requests.get(detail_url,headers=headers) # html = resp.text # soup = BeautifulSoup(html,&#x27;lxml&#x27;) # name = list(soup.find(&#x27;div&#x27;,id=&#x27;content&#x27;).find(&#x27;h1&#x27;).stripped_strings) # title = list(soup.find(&#x27;span&#x27;,property=&#x27;v:itemreviewed&#x27;).stripped_strings) # name = &#x27;&#x27;.join(name) # 将列表转化为字符串 # director = list(soup.find(&#x27;a&#x27;,rel=&#x27;v:directedBy&#x27;)) # director = list(soup.find(&#x27;div&#x27;,id=&#x27;info&#x27;).find(&#x27;span&#x27;,class_=&#x27;attrs&#x27;).stripped_strings) # print(director) # writer = list(soup.find(&#x27;div&#x27;,id=&#x27;info&#x27;).find_all(&#x27;span&#x27;)[3].find(&#x27;span&#x27;,class_=&#x27;attrs&#x27;).stripped_strings) # writer = &#x27;&#x27;.join(writer) # print(writer) # actor = list(soup.find(&#x27;div&#x27;,id=&#x27;info&#x27;).find(&#x27;span&#x27;,class_=&#x27;actor&#x27;).stripped_strings) # actor = &#x27;&#x27;.join(actor) # print(actor) # remark = list(soup.find(&#x27;span&#x27;,class_=&#x27;all hidden&#x27;).stripped_strings) # print(remark) # score = list(soup.find(&#x27;strong&#x27;,class_=&#x27;ll rating_num&#x27;).stripped_strings) # print(score)if __name__ == &#x27;__main__&#x27;: main() 好家伙，我写这个文档时候，为了要上面那个“状态异常”的截图，把cookie删了以后疯狂爬取，直接导致我被封号了： 而且好像还没那么容易解锁。 哎，先看最后一个吧 selenium行为链实战 有些厉害的网站会根据鼠标行为判断你是人还是爬虫，这个时候selenium库的行为链就可以帮你的爬虫躲过检查。因为我还没遇到这么牛逼的网站，所以就随便举个例子，说一下行为链的语法了。 比如我想在百度主页搜索“元尊” 123456789101112131415161718192021from selenium import webdriverfrom selenium.webdriver.common.action_chains import ActionChainsdriver = webdriver.Chrome(executable_path=&quot;/Users/pangyuxuan/Desktop/chromedriver&quot;)driver.get(&quot;https://www.baidu.com/&quot;)inputTag = driver.find_element_by_id(&#x27;kw&#x27;) # 获取输入框submitTag = driver.find_element_by_id(&#x27;su&#x27;) # 获取按钮&quot;百度一下&quot;actions = ActionChains(driver) # 创建行为链对象actionsactions.move_to_element(inputTag) # 鼠标移动到inputTagactions.send_keys_to_element(inputTag,&#x27;元尊&#x27;) # 输入要搜索的内容actions.move_to_element(submitTag) # 鼠标移动到提交按钮actions.click(submitTag) # 点击提交actions.perform() # 上面的都是在定义，还没执行，通过perform执行 还有更多的鼠标相关的操作。 click_and_hold(element) ：点击但不松开鼠标。 context_click(element) ：右键点击。 double_click(element) ：双击。 更多方法请参考：http://selenium-python.readthedocs.io/api.html 彩蛋：关于我被封号这件事","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/"},{"name":"实战","slug":"爬虫/实战","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"常用LaTex公式及用法","slug":"latex-toolbook","date":"2022-08-06T15:46:36.000Z","updated":"2022-08-06T15:55:48.414Z","comments":true,"path":"2022/08/06/latex-toolbook/","link":"","permalink":"https://pyxblog.cn/2022/08/06/latex-toolbook/","excerpt":"","text":"运算符 常用二元运算 \\pm \\cdot \\times \\mp \\div \\frac{1}{1} 常用巨算符 \\sum \\prod \\int \\bigcup \\bigcap \\oint 关系符 常用二元关系 \\leq or \\le \\geq or \\ge \\ll \\gg \\in \\subset \\subseteq \\equiv \\approx \\propto 取反 加\\not来实现取反，如:\\not \\subset 特殊字符 常用希腊字母 \\alpha \\beta \\gamma \\delta \\epsoilon \\zeta \\theta \\lambda \\pi \\xi \\rho \\sigma \\eta \\phi \\psi \\omega 将首字母大写即可获得相应大写字母 特殊符号 特殊符号 \\{ \\} \\% \\dots \\cdots \\vdots \\to \\Rightarrow \\varnothing \\forall \\exists \\infty 公式块的相关写法 等式对齐写法 开头写\\begin{aligned}，结尾写\\end{aligned}，中间需要对其的等式用&amp;&amp;包裹，再换行\\\\ 例如 12345$$\\begin{aligned} S(n) &amp; = 1+2+ \\dots +n &amp; \\\\ &amp; = \\frac{(n+1) \\times n}{2} &amp;\\end{aligned}$$ 大括号的用法 开头写\\begin{cases}，结尾写\\end{cases}，中间如下，例如 1234567$$1+(-1)^k=\\begin{cases}2 &amp; \\text{k=2n-1}\\\\0 &amp; \\text{k=2n}\\end{cases}$$ 组合数的写法 可以强行写： 123$$C_n^m$$ 也可以使用大括号的描写方式，注意上面的是范围，下面的数是要取的个数，与的上下相反 123$$\\tbinom{n}{m}$$ 求和符号的行内写法 若直接写 1$\\sum_{i=0}^n a_n$ 会显示为 若想将和挪到的上下，可以如下写 1$\\sum\\limits_{i=0}^n a_n$ 效果： 矩阵的写法 123456\\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{matrix} \\\\\\begin{pmatrix} 0 &amp; -i \\\\ i &amp; 0 \\end{pmatrix} \\\\\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix} \\\\\\begin{Bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{Bmatrix} \\\\\\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix} \\\\\\begin{Vmatrix} i &amp; 0 \\\\ 0 &amp; -i \\end{Vmatrix} 大佬的文章备查 LaTex符号大全-基于lshort-zh-cn 转载自王美庭 2019-03-26 Latex所有常用数学符号吐血整理！（包含大括号、等式对齐） 原创繁凡さん 2020-05-27 常用LaTex表达式&amp;符号——组合数学篇 原创__hep__ 2020-05-03","categories":[{"name":"markdown系列教程","slug":"markdown系列教程","permalink":"https://pyxblog.cn/categories/markdown%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"latex","slug":"latex","permalink":"https://pyxblog.cn/tags/latex/"},{"name":"markdown","slug":"markdown","permalink":"https://pyxblog.cn/tags/markdown/"}]},{"title":"pytorch实现线性回归","slug":"pytorch-demo-2","date":"2022-08-06T15:42:03.000Z","updated":"2022-08-06T15:44:48.720Z","comments":true,"path":"2022/08/06/pytorch-demo-2/","link":"","permalink":"https://pyxblog.cn/2022/08/06/pytorch-demo-2/","excerpt":"","text":"用cpu就能很快跑出来 不需要额外的输入文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import torchimport numpy as npimport torch.nn as nnx_values = [i for i in range(11)] # [0,1,2,3,4,5,6,7,8,9,10]x_train = np.array(x_values, dtype=np.float32)x_train = x_train.reshape(-1, 1) # 将x_train调整为11*1的矩阵y_values = [2.5*i+3.5 for i in x_values] # y=2.5x+3.5y_train = np.array(y_values, dtype=np.float32)y_train = y_train.reshape(-1, 1)class LinearRegressionModel(nn.Module): # 继承自nn包的Module类 def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() # 执行父类的构造函数 self.linear = nn.Linear(input_dim, output_dim) # nn.Linear(输入数据维度, 输出数据维度) 全连接层 def forward(self, x): out = self.linear(x) return outinput_dim = 1output_dim = 1model = LinearRegressionModel(input_dim, output_dim)epochs = 1000 # 训练次数learning_rate = 0.01 # 学习率optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)# 优化器，使用基本的优化器SGD，传入需要更新的参数（model中的全部参数）和学习率criterion = nn.MSELoss() # 回归任务可选用MSE等for epoch in range(epochs): epoch += 1 # x_train和y_train均为numpy.ndarry格式，需要转换为tensor格式才可以传入框架 inputs = torch.from_numpy(x_train) labels = torch.from_numpy(y_train) # 每次迭代开始时 梯度需要清零 optimizer.zero_grad() # 前向传播 outputs = model.forward(inputs) # 计算损失 loss = criterion(outputs, labels) # 反向传播 loss.backward() # 更新权重参数 optimizer.step() # 每50个epoch输出一次，以显示训练进度 if epoch % 50 == 0: print('epoch {}, loss {}'.format(epoch, loss.item()))y_predicted = model.forward(torch.from_numpy(x_train)).data.numpy()# 前向传播 传入训练数据x 输出预测结果y 用以测试# .data.numpy() 将结果转换成numpyprint(y_predicted)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://pyxblog.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"实战demo","slug":"深度学习/实战demo","permalink":"https://pyxblog.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E6%88%98demo/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"},{"name":"PyTorch","slug":"PyTorch","permalink":"https://pyxblog.cn/tags/PyTorch/"},{"name":"深度学习","slug":"深度学习","permalink":"https://pyxblog.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"pytorch实现Minist手写数字识别","slug":"pytorch-demo-1","date":"2022-08-06T15:30:43.688Z","updated":"2022-08-06T15:44:45.804Z","comments":true,"path":"2022/08/06/pytorch-demo-1/","link":"","permalink":"https://pyxblog.cn/2022/08/06/pytorch-demo-1/","excerpt":"","text":"代码中的 import xxx 和 from xxx import xxx 为依赖包，可按编译器的提示自行安装。 数据集下载地址：http://deeplearning.net/data/mnist/ 考虑到外网下载较慢，提供国内的镜像下载链接： Github仓库：https://github.com/zionfuo/keras-datasets 数据集下载后，代码中读取文件： 12345with gzip.open(&#x27;data/mnist/mnist.pkl.gz&#x27;, &#x27;rb&#x27;) as f: # 读取数据 ((x_train, y_train), (x_test, y_test), _) = pickle.load(f, encoding=&#x27;latin-1&#x27;)x_train, y_train, x_test, y_test = map( # 将数据类型转换为tensor torch.tensor, (x_train, y_train, x_test, y_test)) 注意将上面的文件路径换成自己数据集的路径。下面是完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# -- 构造网络 --# mnist数据集50000个样本，每个样本28*28=784个像素点# 输入数据1*784，设计三层网络，第一层784*128，第二层128*256，第三层256*10from torch import nnimport torch.nn.functional as Fclass Mnist_NN(nn.Module): # 继承父类 def __init__(self): super().__init__() # 继承父类的构造函数 self.hidden1 = nn.Linear(784, 128) # 隐层1 784*128 self.hidden2 = nn.Linear(128, 256) # 隐层2 128*256 self.out = nn.Linear(256, 10) # 输出层 def forward(self, x): # 前向传播 x = F.relu(self.hidden1(x)) # 输入-隐层1，激活函数relu x = F.relu(self.hidden2(x)) # 隐层1-隐层2，激活函数relu x = self.out(x) # 隐层2-输出 return x # 返回前向传播计算结果# -- 构造数据集 --import gzipimport pickleimport torchfrom torch.utils.data import TensorDataset # 用于创建数据集from torch.utils.data import DataLoader # 用于加载数据集# 使用TensorDataset和DataLoader创建的数据集可以根据传入的batch自动抽样# 也可自动在每次分组时洗牌with gzip.open(&#x27;data/mnist/mnist.pkl.gz&#x27;, &#x27;rb&#x27;) as f: # 读取数据 ((x_train, y_train), (x_test, y_test), _) = pickle.load(f, encoding=&#x27;latin-1&#x27;)x_train, y_train, x_test, y_test = map( # 将数据类型转换为tensor torch.tensor, (x_train, y_train, x_test, y_test))train_dataset = TensorDataset(x_train, y_train) # 训练数据集test_dataset = TensorDataset(x_test, y_test) # 测试数据集def getData(train_dataset, test_dataset, batch_size): # 加载数据集 return ( DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size * 2), )# -- 训练 --import numpy as npfrom torch import optimloss_func = F.cross_entropy # 损失函数，直接从functional中调用交叉熵函数def getModel(): # 获取实例化模型和优化器 model = Mnist_NN() return model, optim.SGD(model.parameters(), lr=0.001)def loss_batch(model, loss_func, x_bath, y_bath, opt=None): loss = loss_func(model(x_bath), y_bath) # 有优化器，即训练，需要进行更新参数等操作 # 无优化器，即测试，只求损失值即可 if opt is not None: loss.backward() # 反向传播 opt.step() # 更新参数 opt.zero_grad() # 梯度清零 return loss.item(), len(x_bath)def mnist(steps, model, loss_func, opt, train_data, test_data): # steps 迭代多少次 # model 网络的实例 # loss_func 损失函数 # opt 优化器 # train_data 训练数据 # test_data 测试数据 for step in range(steps): for x_bath, y_bath in train_data: # 训练 loss_batch(model, loss_func, x_bath, y_bath, opt) with torch.no_grad(): # 测试，不更新参数 losses, nums = zip( *[loss_batch(model, loss_func, x_bath, y_bath) for x_bath, y_bath in test_data] ) val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums) print(&#x27;当前step:&#x27; + str(step), &#x27;验证集损失：&#x27; + str(val_loss)) 训练： 123train_data, test_data = getData(train_dataset, test_dataset, 64)model, opt = getModel()mnist(25, model, loss_func, opt, train_data, test_data)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://pyxblog.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"实战demo","slug":"深度学习/实战demo","permalink":"https://pyxblog.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E6%88%98demo/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"},{"name":"PyTorch","slug":"PyTorch","permalink":"https://pyxblog.cn/tags/PyTorch/"},{"name":"深度学习","slug":"深度学习","permalink":"https://pyxblog.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"python快速入门","slug":"python-basic","date":"2022-08-06T15:27:25.000Z","updated":"2022-08-06T15:30:10.360Z","comments":true,"path":"2022/08/06/python-basic/","link":"","permalink":"https://pyxblog.cn/2022/08/06/python-basic/","excerpt":"","text":"数值类型与基本操作 12345678910112**5 # 2的5次方1.3e5 1.3e-5 # 科学计数法0xFF # 16进制 0x是16进制前缀type(a) # 打印数据类型a = int(input('请输入数据')) # 类型转换# input()输入数据默认为字符串# 常见类型 int float str bool list dict-字典 set-集合abs(5.2) # abs函数——取绝对值round(5.2) # round函数——四舍五入min(2,3,4) # min函数——取最小值 可以有若干参数max(2,3,4) # max函数——取最大值 可以有若干参数 基本数据结构 字符串 str 123456789101112131415161718192021222324252627282930313233343536st = 'jupyter ' + 'python' # 字符串加法 -&gt; 有序拼接 'jupyter python'st = 'jupyter ' * 3 # 字符串数乘 -&gt; 重复 'jupyter jupyter jupyter'len(st) # 字符串长度# 拆分st = 'Gin Vodka Vermouth Bourbon'st.split() # 默认按空格拆分字符串 返回列表 ['Gin','Vodka','Vermouth','Bourbon']st.split('Vodka') # 也可指定拆分间隔 间隔不会计入结果# 合并li = ['Gin','Vodka','Vermouth','Bourbon']st = '\\\\'st.join(li) # 以st为衔接合并列表，要求列表中的元素均为字符串类型 # 'Gin\\\\Vodka\\\\Vermouth\\\\Bourbon'# 替换st = 'Gin Vodka Vermouth Bourbon'st.replace('Bourbon','Zero')# 大小写st = 'Vermouth'st.lower() # vermouthst.upper() # VERMOUTHst.strip() # 去除首尾多余空格st.lstrip() # 去除左边多余空格st.rstrip() # 去除右边多余空格# format占位'{} and {} are black'.format('Gin','Vodka')# Gin and Vodka are black'{2} and {1} are black'.format('Gin','Vodka') # 指定索引# Vodka and Gin are black'{a} and {b} are black'.format(a='Gin',b='Vodka') # 指定参数# Gin and Vodka are black 列表 list 有序，可以混合存放任意类型数据，可嵌套，通过索引访问 123456789101112131415161718192021222324li = [1,2] + [3,4] # 列表相加 -&gt; 有序拼接 [1,2,3,4]li = [1,2] * 3 # 列表数乘 -&gt; 重复 [1,2,1,2,1,2]len(li) # 求列表长度del li[3:] # 按索引删除某元素tmp in li # 判断元素tmp是否在列表li中 返回True or Falseli.count(tmp) # 计算元素tmp在列表中的个数li.index(tmp) # 返回元素tmp在列表中首次出现的索引 没找到报错ValueErrorli.append(tmp) # 将元素tmp添加进列表最后li.insert(ind,tmp) # 将元素tmp添加在列表索引ind位置li.remove(tmp) # 将列表中首个tmp从列表中移除li.pop() # 将列表尾部的元素弹出 并返回该元素li.pop(ind) # 将索引ind位置的元素弹出 并返回该元素li.sort() # 列表升序排序li.sort(reverse=1) # 列表降序排序li_sort = sorted(li) # 列表升序排序 将结果储存在新列表中 不改变原列表li.reverse() # 列表翻转# 按索引遍历for i in range(len(li)): print(li[i])# 按元素遍历for t in li: print(t) 索引 用于有序数据结构，如字符串和列表，不用于字典和集合 123456789101112# 首端正数从0开始，尾端倒数从-1开始st = '01234567'st[0] # '0'st[-1] # '7' # 切片st[0:4] # 左闭右开区间 '0123'st[4:] # 4到无穷 '4567'st[:4] # 0到4 '0123'st[-2:] # 倒数第二到无穷 '67'st[::3] # 每3个取一个值 '036' 字典 dict 基本结构：key-value 无序，使用key访问value，不可使用索引 12345678910111213141516171819202122232425di = {'Gin':'black','Bourbon':'red'}di['Gin'] # 'black'# 列表转化为字典di = dict([('amy',89), ('tom',90)])# {'amy': 89, 'tom': 90}di.get('amy') # 89 di.get('sam') # 没找到 但不会报错di.get('sam','none') # 返回nonedi.pop('amy') # 弹出指定key-value 并返回valuedi['tom'] +=10 # {'tom',100} value可以被运算del di['tom'] # 删除指定key-valuedi = {'Gin':'black','Bourbon':'red'}di.update({'Gin':'black','Vodka':'black'}) # 更新'Gin' in di # Truedi.keys() # 返回字典di的所有key 类型为dict_keys 可用于遍历di.values() # 返回字典di的所有value 类型为dict_values 可用于遍历tang.items() # 返回字典di的所有key-value对# 遍历for key in di.keys(): print(di[key]) 集合 set 集合内元素不重复，无序，可以进行一些集合间的数学运算/判断 123456789101112131415li = [1,1,2,3,5,8]se = set(li) # {1,2,3,5,8}li = list(se) # 与list可以互相转化a = {1,2,3,4}b = {2,3,4}a | b # 并集 {1,2,3,4}a &amp; b # 交集 {2,3,4}a - b # 在a里面 不在b里面的元素 {1}b &lt;= a # True 判断子集a &lt;= a # Truea.update([4,5,6]) # 更新 {1,2,3,4,5,6}a.remove(1) # 移除指定元素a.pop() # 弹出并返回首部元素 逻辑结构 判断结构 通过and/or连接多个判断条件 三个及以上判断结果用elif表示 使用: 和缩进表示逻辑从属 123456789a = 90if a &gt; 100 and a &lt;= 200: print('if')elif a == 50 or a == 90: print('elif1')elif (a &lt;10 and a &gt; 0) or a &gt; 200: print('elif2')else: print('else') 循环结构 while 循环 12345678a = 0while a &lt; 5: print(a) a += 1se = {'Gin','Vodka','Bourbon'}while se: print(se.pop()) for循环 1234567for i in range(1,5): print(i)di = {'Gin':'black','Vodka':'black','Bourbon':'red'}for key in di.keys(): print(di[key]) 函数 参数和返回值根据需要设定，通过def关键字、:和缩进表示逻辑从属 12345678910111213141516171819202122def print_value(a): print('The value of a is ',a) def add_value(a=1,b=2): # 为参数设置默认值 print('a+b is ',a+b) def add_number(a,*args): # 接受不定个数的参数 b = 0 for i in args: a += i b += a return a,b # 返回值可以有多个a,b = add_number(1,2,3)print (a,b) # 6 9 def add_2(a,**kwargs): # **kwargs可传入不定个数的key-value对 for arg,value in kwargs.items(): print(arg,value)add_2(1,x=2,y=3)# x 2# y 3 包 12345678%%writefile test.py # writefile用于在jupyter中新建文件value = 10010def test_function(): print('success') 123456789101112# 导入包import test as te # 整个导入包 调用时需用te.print(te.value) # 10010te.test_function() # successfrom test import value,test_function # 导入包中的部分变量和函数 直接使用无需.print(value) # 10010te.test_function() # successfrom test import * # 导入包中的部分变量和函数 直接使用无需.print(te.value) # 10010te.test_function() # success 类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class people: '帮助信息：这是一个帮助信息' # 类的自带属性 location = 'earth' # 所有实例都会共享number def __init__(self,name,age): # 构造函数 self.name = name self.age = age def show_name(self): print(self.name)per1 = people('Vodka',40) # 实例化per1.name = 'Gin' # 访问per1的name变量per2.show_name() # 访问per2的show_name函数del per1.name # 删除实例per1的name属性hasattr(per1,'age') # True 查看实例是否具有某一属性hasattr(per1,'name') # False 被删除的属性和不存在的属性返回Falsegetattr(per1,'age') # 获取实例per1的属性agesetattr(per1,'name','Gin') # 为实例per1设置name属性setattr(per1,'sex','male') # 可以设置类中没有的属性delattr(per1,'sex') # 删除per1的sex属性print(peolpe.__doc__) # 帮助信息print (people.__name__) # 类的名字print (people.__module__) # 类的定义所在的模块print (people.__bases__) # 父类的构成print (people.__dict__) # 类的组成# 继承class Dad: # 父类 def __init__(self): print ('父类构造函数') def dadFunction(self): print ('父类方法') def sameName(self): print ('来自dad') class son(Dad): # Dad的子类 def __init__(self): print ('子类构造函数') def sonFunction(self): print ('子类方法') def sameName(self): print ('来自子类')child = son() # 子类构造函数child.dadFunction() # 父类方法child.sonFunction() # 子类方法child.sameName() # 来自子类 基础操作 异常处理 用以防止报错导致的程序中断 12345678910111213141516171819202122232425262728293031323334353637383940414243444546li = [1,0]for tmp in li: try: print('front') # 报错语句前的语句可以被执行 print(1/tmp) # 报错 跳转至except 该语句不会被执行 print('last') # 报错语句后的语句不可以被执行 except ZeroDivisionError: print('不可以除0') # 涵盖所有报错类型import mathfor i in range(10): try: input_number = input('write a number') if input_number == 'q': break result = 1/math.log(float(input_number)) print (result) except ValueError: print ('ValueError: input must &gt; 0') except ZeroDivisionError: print ('log(value) must != 0') except Exception: # 其他的可能性 print ('unknow error') # finallytry: 1 / 0except: print('不可以除0')finally: print('finally') # 无论try中是否有异常，finally都会被执行 # 自定义错误类型class NumNotInList(ValueError): # 自定义的一种异常 passli = [1,2,3]while True: num = int(input('input a number: ')) if num not in li: raise NumNotInList('数字{}不在列表中'.format(num)) # 抛出一个错误 输入不在列表中的数字时报错终止 并显示错误类型为NumNotInList 文件处理 1234567891011121314151617181920txt = open('./test.txt') # ./ 即本代码所在的路径txt_str = txt.read() # read函数返回文本内容txt_lines = txt.readlines() # readlines返回列表 文本每一行组成一个元素 包括换行符txt.close() # 文件打开后要记得关闭txt = open('test.txt','w') # 以覆盖写入模式打开文件 一旦写入丢失原有数据txt.write('123')txt.close()txt = open('test.txt','a') # 以追加写入模式打开文件 在原有数据后面写入新的数据txt.write('\\n321')txt.close()txt = open('test.txt','r') # 以只读模式打开文件 可以读取文件内容print (txt.read())txt.close()with open('test.txt','w') as f: f.write('123\\n321') # with方法会自动close 且自带类似try-except的防止报错功能 较为常用 系统时间 所在包：time.py 123456789import timeprint(time.time()) # 从1970年1月1日到现在经过了多长时间print (time.localtime(time.time()))# time.struct_time(tm_year=2022, tm_mon=2, tm_mday=4, tm_hour=23, tm_min=18, tm_sec=46, tm_wday=4, tm_yday=35, tm_isdst=0)print (time.asctime(time.localtime(time.time())))# Fri Feb 4 23:18:38 2022print (time.strftime('%Y-%m-%d %H:%M:%S',time.localtime()))# 2022-02-04 23:19:17time.sleep(10) # 程序停止十秒","categories":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/categories/python/"},{"name":"基础教程","slug":"python/基础教程","permalink":"https://pyxblog.cn/categories/python/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"}]},{"title":"Matlab二维绘图","slug":"matlab-basic-6","date":"2022-08-06T15:11:00.000Z","updated":"2022-08-06T15:26:06.922Z","comments":true,"path":"2022/08/06/matlab-basic-6/","link":"","permalink":"https://pyxblog.cn/2022/08/06/matlab-basic-6/","excerpt":"","text":"基本绘图函数 plot 最基本的绘图函数。执行plot函数时，若当前已有图形窗口，则将图画在现有图形窗口上，覆盖原有图形；若当前没有图形窗口，则自动创建新的图形窗口。 ```matlab plot(X,Y); % 创建以X为自变量、Y为因变量的二维线图 123456789`X`和`Y`必须是**同维向量**，绘制以`X`为横坐标、`Y`为纵坐标的曲线。若：* **`X`是向量，`Y`是矩阵，则`X`的维数应与`Y`的某一维相等**，绘制多条颜色不同的曲线，曲线数等于`Y`的另一维数，`X`则仍作为横坐标。当`Y`是方阵时，**Matlab会优先处理列**，即绘制`Y`的每一列对`X`的曲线。 ```matlab x = 0:0.01:10; % 1*1001 y = [sin(x);sin(x)+1;sin(x)-1]; % 3*1001 plot(x,y); grid on; % 3条曲线 &lt;img src=&quot;https://img-1306037672.cos.ap-beijing.myqcloud.com/image-20220221110402401.png&quot; alt=&quot;image-20220221110402401&quot; style=&quot;zoom:33%;&quot; /&gt; X是矩阵，Y是向量时，规则同上，但Y会被当作横坐标 X与Y皆为矩阵时，要求二者必须同维，以X的每一列作为横坐标、以Y对应对列元素作为纵坐标绘制曲线，曲线数等于列数。 ```matlab plot(x,y,LineSpec); % 设置线型、标记符号和颜色 plot(x,y,'--or'); % 带有圆形标记的红色虚线 123456789101112131415161718192021222324252627282930313233343536373839404142 `LineSpec`是一个字符串，可以包含上述若干要素，基本线型、标记和颜色如下： | 线型 | 说明 | | :----------: | :----: | | `-(default)` | 实线 | | `--` | 虚线 | | `:` | 点线 | | `-.` | 点划线 | | 颜色 | 说明 | | :--: | :--------: | | `w` | 白 white | | `y` | 黄 yellow | | `c` | 青 cyan | | `g` | 绿 green | | `m` | 品 magenta | | `r` | 红 red | | `b` | 蓝 blue | | `k` | 黑 black | | 标记 | 说明 | | :--: | :---------------: | | `o` | 圆圈 | | `+` | 加号 | | `*` | 星号 | | `.` | 点 | | `x` | 叉号 | | `s` | 方形 square | | `p` | 五角形 pentagonal | | `d` | 菱形 diamond | | `^` | 上三角 | | `v` | 下三角 | | `&gt;` | 右三角 | | `&lt;` | 左三角 | | `h` | 六角形 hexagon | ps. 如果仅指定标记而忽略线型，则绘图时不会显示线条，只显示标记。* ```matlab plot(x1,y1,...,xn,yn); % 在同一个坐标区域内绘制多张图 plot(x1,y1,LineSpec1,...,xn,yn,LineSpecn); 也可以使用hold on命令，将不同的图画在同一个坐标系里。 ```matlab plot(y); % 创建以y中数据为因变量、相应索引为自变量的二维线图 plot(y,LineSpec) 123456789 * 若`y`是实向量，则x轴刻度范围为$[1,length(y)]$ * 若`y`是实矩阵，则按列绘制曲线，相当于`plot(索引矩阵,y)`，自变量为索引 * 若`y`是复矩阵，则按列绘制曲线，相当于`plot(real(y),imag(y))`，**自变量为实部，因变量为虚部*** ```matlab plot(ax,_); % 指定坐标范围 plot(_,Name,Value); % 使用一个或多个(Name,Value)对，单独指定某些属性的值 h = plot(_); % 用变量h将图形储存，可以通过改变h的属性来实时修改图形 图形线条属性可以通过输出h来查看： 123456789 Color: [0.8500 0.3250 0.0980] % 颜色 LineStyle: &#x27;-&#x27; % 线型 LineWidth: 0.5000 % 线宽 Marker: &#x27;none&#x27; % 符号 MarkerSize: 6 % 符号大小MarkerFaceColor: &#x27;none&#x27; % 符号填充颜色 XData: [1×1001 double] % x轴数据 YData: [1×1001 double] % y轴数据 ZData: [1×0 double] % z轴数据 修改示例如下： 123x = 0:0.01:10;y = [sin(x);cos(x)];h = plot(x,y); 12&gt;&gt; h(2).YData = 0.1*x; % 修改第二条线的y轴数据&gt;&gt; h(1).LineStyle = &#x27;--&#x27;; % 修改第一条线的线型 此外，还有一些常用的属性如下 属性 说明 值 LineJoin 线条边角样式 round(default) miter chamfer MarkerEdgeColor 标记轮廓颜色 none(default) auto 颜色 fplot 专门用于绘制一元函数的命令。相比于plot()根据指定数据点绘图，fplot()会自适应地选取数据点，即在平滑处选取数据点稀疏、在陡峭处选取数据点密集，使图像更加光滑准确。 ```matlab fplot(f); % 在x的默认区间[-5,5]绘制由函数y=f(x)定义的曲线 fplot(f,x_interval); % 在x的指定区间x_interval绘制由函数y=f(x)定义的曲线 12345`f` 为m文件函数名或系统自带函数名，`x_interval`为一个二元向量，包含区间的两个端点。示例如下：```matlabfplot(@exp,[-1,1]); grid on; ```matlab fplot(funx,funy); % 在t的默认区间[-5,5]绘制由参数方程x=funx(t),y=funy(t)定义的曲线 fplot(funx,funy,t_interval); % 在t指定区间t_interval绘制由参数方程x=funx(t),y=funy(t)定义的曲线 1234567`funx`和`funy`为参数方程函数名，`t_interval`为一个二元向量，包含区间的两个端点。```matlabxt = @(t) 2*cos(t);yt = @(t) sin(t);fplot(xt, yt); grid on; ```matlab fplot(,LineSpec); % 指定线型、标记符号和颜色 fplot(,Name,Value); % 指定线条属性 fplot(ax,); % 指定坐标范围 fp = fplot(); % 返回可修改的图形对象 123456789****### subplot用于在同一个图形窗口中分割出多个视图区域。```matlabsubplot(m,n,p); % 将当前窗口分割成m行n列的区域，用p指定当前位置 对p的编号采用从左至右、从上至下的原则。 12subplot(m,n,p,&#x27;replace&#x27;); % 将视图替换为空坐标区subplot(&#x27;Position&#x27;,pos); % 在pos指定的自定义位置创建坐标区 pos的格式为[left, bottom, width, height]，即以左下角坐标、宽度和高度定义，若新坐标区与原有坐标区重叠，则原有坐标区会被替换。 不同坐标系下的绘图 上述所有绘图命令均建立在平面直角坐标系中，下面介绍几种其他坐标系的绘图方法。 极坐标系 polarplot 与直角坐标系的plot函数几乎一致，只是将x换做theta，将y换做rho 12345678polarplot(theta,rho); % theta-极角(rad),rho-极半径polarplot(theta,rho,LineSpec);polarplot(theta1,rho1,...,thetan,rhon);polarplot(theta1,rho1,LineSpec1,...,thetan,rhon,LineSpecn);polarplot(z); % rho对应复数z的模长，theta对应幅角主值polarplot(z,LineSpec);ploarplot(_,Name,Value);p = polarplot(_); 示例： 1234theta = 0:0.01*pi:6*pi;rho1 = theta/10;rho2 = theta/12;polarplot(theta,rho1,&#x27;-b&#x27;,theta,rho2,&#x27;--r&#x27;); 坐标转化： 1234[theta, rho] = cart2pol(x, y); % 直角坐标转极坐标[x, y] = pol2cart(theta, rho); % 极坐标转直角坐标R = deg2rad(D); % 角度转弧度D = rad2deg(R); % 弧度转角度 对数坐标系 对于某些变化迅速的变量，线性坐标可能无法形象展示其变化过程。若将部分或全部坐标取对数，就可以减缓变量的变化过程。常用的对数坐标系有： semilogx() - x轴为对数坐标，y轴为线性坐标 semilogy() - y轴为对数坐标，x轴为线性坐标 loglog() - x、y轴均为对数坐标 函数调用规则与plot类似。示例如下： 1234x = 0:0.01:10;y = exp(x);subplot(2,1,1); plot(x,y); grid on; % 直接绘制subplot(2,1,2); semilogy(x,y); grid on; % 对y轴取对数 ps. 对数以10为底 双y轴坐标系 对于同一个坐标系内的两条曲线，若二者的变化范围差距过大，会导致变化范围较小的曲线无法清晰显示。此时，可以使用yyaxis left与yyaxis right命令为坐标系创建两个y轴，并分别绘制。该命令仅起到定位作用，与subplot类似。 示例如下： 1234567x = 0:0.01:10;y_large = sin(x);y_small = 0.1*cos(x);subplot(2,1,1); plot(x,y_large,x,y_small); grid on;subplot(2,1,2); grid on;yyaxis left; plot(x,y_large);yyaxis right;plot(x,y_small); 图形窗口 Matlab的图形窗口和命令行窗口是相互独立的，通过图形窗口可以修改和编辑图形界面、实现大量数据计算结果的可视化。 创建 使用figure命令创建图形窗口 12345figure % 创建一个图形窗口figure(Name,Value); % 使用(Name,Value)对来修改属性，如(&#x27;Name&#x27;,&#x27;图1&#x27;)f = figure(_); % 使用变量f储存窗口对象，可以通过它改变窗口的属性figure(f); % 指定当前绘图窗口为ffigure(num); % 创建一个编号为num的图形窗口 相关命令 命令 说明 set(f,[Name1,...],[Value1,...]) 设定图形窗口的属性值 get(f) 获取图形窗口的属性值 close close all 关闭图形窗口 clf 清空图形窗口（不会关闭） 图形标注 坐标轴范围 使用axis(limit)指定当前坐标区的范围，limit只能是长度为4、6、8的向量。 123axis([Xmin,Xmax,Ymin,Ymax]); % 2维axis([Xmin,Xmax,Ymin,Ymax,Zmin,Zmax]); % 3维axis([Xmin,Xmax,Ymin,Ymax,Zmin,Zmax,Cmin,Cmax]); % 4维 图形注释 123456fill(x,y,&#x27;color&#x27;); % 用指定颜色填充数据(x,y)构成的多边形title(&#x27;string&#x27;); % 为图形添加标题xlabel(&#x27;string&#x27;); % 为x轴添加标注ylabel(&#x27;string&#x27;);zlabel(&#x27;string&#x27;);text(x,y,&#x27;string&#x27;); % 在指定位置添加字符串 可以配合num2str(num)函数，为图像添加与数值有关的标注，字符串之间使用[]衔接。 12345x = 0:0.01:10;k = rand(1,1);y = sin(x) * k;plot(x, y);title([&#x27;k=&#x27;, num2str(k)]); % 标题显示随机数k的取值 图例 12345legend(label1,...,labeln); % 按照曲线顺序设置图例legend(_,&#x27;Location&#x27;,lcn); % 指定图例的位置% &#x27;north&#x27;|&#x27;south&#x27;|&#x27;east&#x27;|&#x27;west&#x27;|&#x27;northeast&#x27;|...legend(_,&#x27;Orientation&#x27;,ornt); % 指定图例的显示方式% &#x27;vertical&#x27;(defalut)|&#x27;horizontal&#x27; 网格线 123grid on; % 为当前坐标区添加主网格线grid; % 切换主网格线可见性grid minor; % 切换次网格线可见性 绘制特殊图形 条形图bar 12bar(y); % 创建一个条形图，y中的每个元素对应一个条形。bar([1,2,3,4,5]); 当y是\\(m\\times n\\)的矩阵时，创建\\(m\\)组，每组包含\\(n\\)个条形： 1bar(rand(2,5)); 123456bar(x,y); % 在横坐标x指定的位置绘制y，要求x为严格单调递增的向量bar(_,width); % 设置条形的相对宽度bar(_,style); % 设置条形组的样式 % &#x27;grouped&#x27;(defalut)|&#x27;stacked&#x27;|&#x27;hist&#x27;|&#x27;histc&#x27;bar(_,color); % 设置条形的颜色b = bar(_); % 保存对象，可以修改其属性值 此外，还有其他形式的条形图，调用格式类似： 函数 说明 barh() 水平条形图 bar3() 竖直三维条形图 bar3h() 水平三维条形图 区域图area 12345area(x); % 与plot(x)一致，但会将曲线下方区域填充颜色area(x,y); % 与plot(x,y)一致，但会将曲线下方区域填充颜色area(x,Y); % 矩阵Y按列对向量x绘图，图像依次累加area(_,basevalue); % 指定区域填充的基值，默认为0ar = area(_); % 保存对象，可以修改其属性值 123456x = 0:0.5:5;Y = [ones(size(x)) rand(size(x))+1 rand(size(x))+1 rand(size(x))+1];area(x,Y&#x27;,-1); % 矩阵Y的行数须与向量x一致，指定基值为-1 饼图pie 12345pie(x); % 使用x中的数据绘制饼图pie(x,explode); % 将扇区从饼图偏移一定位置% explode与向量x长度相同，其中的值分别对应偏移大小pie(x,labels); % 指定扇区的文本标签，标签数必须等于向量x的长度，采用元胞表示pie(x,explode,labels); 1234x = [1, 3, 1, 5];explode = [0, 0.1, 0.2, 0.3];labels = &#123;&#x27;无偏移&#x27;, &#x27;偏移0.1&#x27;, &#x27;偏移0.2&#x27;, &#x27;偏移0.3&#x27;&#125;;pie(x, explode, labels); 可以用pie3绘制三维饼图。 直方图histogram与polarhistogram 通过help指令查询详细信息。 12x = randn(10000, 1);histogram(x); 12theta = [0.1 1.1 5.4 3.4 2.3 4.5 3.2 3.4 5.6 2.3 2.1 3.5 0.6 6.1];polarhistogram(theta,6); 含误差的线图errorbar 123456errorbar(y,err); % 创建y中数据的线图，并在每个数据点绘制一个垂直误差条% err和y长度相同，对应了每个数据点的误差大小errorbar(x,y,err); % 横坐标xerrorbar(x,y,neg,pos); % neg确定数据点向下误差，pos确定数据点向上误差errorbar(_,ornt); % 设置误差条的方向 &#x27;vertical&#x27;(default)|&#x27;horizontal&#x27;|&#x27;both&#x27;e = errorbar(_); 1234x = 1:10;y = x;err = [0.1:0.1:0.5, 0.1:0.1:0.5];errorbar(x,y,err,&#x27;both&#x27;); 离散图（针状图）stem 用法与plot一致。 1234y = 1:6;stem(y);hold on;stem(y+1,&#x27;filled&#x27;); % 绘制实心点 可以用stem3绘制三维针状图 阶梯图stairs 用法与plot一致 1234x = 0:0.1:2*pi;stairs(x, sin(x));hold on; grid on;stairs(x, cos(x)); 罗盘图compass 用箭头显示坐标为\\((u,v)\\)的向量，\\(u\\)和\\(v\\)长度一致，箭头起点位于原点。 12compass(u,v);compass(z); % 相当于compass(real(z), imag(z)); 与其他画图函数类似，可以指定线型、标记符号和颜色，可以用对象保存图像。 123u = [1, 0, -3, 0];v = [0, 2, 0, -4];compass(u,v); 箭头图quiver 12quiver(x,y,u,v); % 在(x,y)位置绘制由(u,v)确定的向量quiver(u,v); % 相当于quiver(1:n,1:m,u,v),其中u,v为m*n的矩阵 1234567[x, y] = meshgrid(-2:.2:2); % 返回网格坐标z = x.*exp(-x.^2 - y.^2);[dx, dy] = gradient(z, .2, .2); % 返回梯度contour(x, y, z); % 绘制等高线hold on;quiver(x, y, dx, dy);","categories":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]},{"title":"Matlab符号运算","slug":"matlab-basic-5","date":"2022-08-06T15:06:23.000Z","updated":"2022-08-06T15:09:29.031Z","comments":true,"path":"2022/08/06/matlab-basic-5/","link":"","permalink":"https://pyxblog.cn/2022/08/06/matlab-basic-5/","excerpt":"","text":"符号运算是数值计算的扩展，在运算过程中以符号表达式或符号矩阵为运算对象，实现了符号计算和数值计算的相互结合，使应用更灵活。 创建符号表达式 创建符号表达式，需要先创建符号变量，再使用它们编写表达式。 使用关键字syms创建符号变量： 123456789101112131415161718192021syms a b c % 一次可以创建多个变量，变量之间只能用空格衔接syms A [3 4] % 创建符号矩阵% A =% % [ A1_1, A1_2, A1_3, A1_4]% [ A2_1, A2_2, A2_3, A2_4]% [ A3_1, A3_2, A3_3, A3_4]syms &#x27;A%d%d&#x27; [2 2] % 可以通过占位符%d来改变默认格式% A =% % [ A11, A12]% [ A21, A22]syms M 3 % 3阶方阵% M =% % [ M1_1, M1_2, M1_3]% [ M2_1, M2_2, M2_3]% [ M3_1, M3_2, M3_3] 先将变量创建好，才能将含有该变量字符串转化为符号表达式 123456syms xstr = &#x27;x^3+2*x+1&#x27;; % 不识别2x，即*不可省略S = eval(str); % 将字符串转化为符号表达式% S =% % x^3 + 2*x + 1 也可以通过多项式部分提到的函数ploy2sym(p)，将系数向量转化为符号表达式 12345P = [1 2 2 1];S = poly2sym(P);% S =% % x^3 + 2*x^2 + 2*x + 1 可以通过函数sym(A)将矩阵\\(A\\)转化为符号表达式sym格式。只有符号表达式可以与符号表达式计算，数值表达式无法直接与符号表达式进行计算。 123456A = ones(2,3);S = sym(A) % 2*3 sym% S =% % [ 1, 1, 1]% [ 1, 1, 1] 使用sym()函数处理数值表达式时，应从尽量小的单位入手，以免产生精度上的误差，如 12345678910111213141516171819202122232425&gt;&gt; sym(1/1234567) % 错误 ans = 7650239286923505/9444732965739290427392 &gt;&gt; 1/sym(1234567) % 正确 ans = 1/1234567% ------------------------------------------------&gt;&gt; sym(exp(pi)) % 错误 ans = 6513525919879993/281474976710656 &gt;&gt; exp(sym(pi)) % 正确 ans = exp(pi) 符号矩阵运算 转置 Matlab默认符号属于复数，在使用'求转置时，会自动求出共轭转置。因此若只想求转置，应该使用.' 12345678910111213syms &#x27;A%d%d&#x27; [2 3]% A =% % [ A11, A12, A13]% [ A21, A22, A23]B = A.&#x27;% B =% % [ A11, A21]% [ A12, A22]% [ A13, A23] 行列式 12345678910syms &#x27;A%d%d&#x27; 2% A =% % [ A11, A12]% [ A21, A22]d = det(A)% d =% % A11*A22 - A12*A21 求逆 1inv(A); % A必须是方阵，结果用A中元素表示 求秩 1rank(A); % 返回一个整数 其他 函数 说明 inv(A) 求矩阵的逆，结果用\\(A\\)中的元素表示 rank(A) 求矩阵的秩，返回一个整数 eig(A) 求特征值、特征向量 svd(A) 奇异值分解 jordan(A) Jordan标准形运算 符号运算 因式分解 使用函数factor(S)实现 12345S = poly2sym([1 3 2]); % S = X^2+3*x+2factor(S)% ans =% % [ x + 2, x + 1] 也可用于质因数分解 12S = sym(276);factor(S) % [2 2 3 23] 表达式展开 123syms xS = eval(&#x27;(x+1)*(x+2)&#x27;);expand(S) % x^2 + 3*x + 2 也可以用于三角函数、指数函数、对数函数的展开 123syms x yS = eval(&#x27;sin(x+y)&#x27;);expand(S) % cos(x)*sin(y) + cos(y)*sin(x) 表达式化简 123syms xS = eval(&#x27;sin(x)^2+cos(x)^2&#x27;);simplify(S) % x+1 分式通分 12345syms x yS = eval(&#x27;1/x+1/y&#x27;);[n, d] = numden(S)% n - 分子 n=x+y% d - 分母 d=x*y 代入/计算结果 通过函数subs(S,old,new)实现，返回值仍是sym类型。 12345678910111213syms F m a Ffstr = &#x27;Ff+F&#x27;;S = eval(str);S = subs(S,F,a*m) % 用a*m代换F% S =% % Ff + a*mres = subs(S,[a m Ff],[2 10 15]) % 分别给[a m Ff]赋值为[2 10 15]% res =% % 35","categories":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]},{"title":"Matlab矩阵","slug":"matlab-basic-4","date":"2022-08-06T14:50:17.000Z","updated":"2022-08-06T15:04:04.016Z","comments":true,"path":"2022/08/06/matlab-basic-4/","link":"","permalink":"https://pyxblog.cn/2022/08/06/matlab-basic-4/","excerpt":"","text":"矩阵是由个数排成的行列数表，记成 含有个维行向量，个维列向量。 矩阵创建 创建 - 直接输入 用[]定义矩阵，同一行的元素用,或空格分割，不同行的元素用;或回车分割 矩阵的大小不需要预先定义，若[]中不写元素，表示空矩阵 创建 - 从文件中读取 可以将较为常用的矩阵写入.m文件，调用时直接运行该文件，即可在工作台看到相应矩阵。除此之外，可以使用load命令调用.txt等类型的文件。 创建 - 用函数生成特殊矩阵 可以用某些函数生成具有某些特点的矩阵，常用函数如下 函数 说明 eye(n) eye(m,n) eye(size(A)) 创建指定大小的单位矩阵 ones(n) ones(m,n) ones(size(A)) 创建指定大小矩阵，其中元素均为1 zeros(n) zeros(m,n) zeros(size(A)) 创建指定大小矩阵，其中元素均为0 rand(n) rand(m,n) rand(size(A)) 创建指定大小矩阵，其中元素服从范围内的均匀分布 randn(n) randn(m,n) randn(size(A)) 创建指定大小矩阵，其中元素服从标准正态分布 compan(P) 创建系数向量为的多项式的伴随矩阵 diag(v) 创建以向量中的元素为对角线的对角阵 hilb(n) 创建一个的Hilbert矩阵 magic(n) 创建一个阶幻方 sparse(A) 将矩阵转化为稀疏矩阵形式，即由的非零元素和下标构成稀疏矩阵 矩阵编辑 矩阵拼接 123456789A = [1 2 3 4];B = zeros(2);C = ones(2);D = [A; B C]% D =% % 1 2 3 4% 0 0 1 1% 0 0 1 1 用,或空格衔接，以实现矩阵的横向拼接，如上例中的矩阵与矩阵 用;或换行衔接，以实现矩阵的纵向拼接，如上例中的矩阵与拼接矩阵 引用矩阵中的某些元素 对矩阵的每个维度均指定一个索引，即可引用相应的数据。 索引可以是标量（一个数）、向量（只能包含正整数）和:（表示全部）。 12345678910A = [1 2 3 4 5 6 7 8 9 10 11 12];A(2,2) % 第2行第2列，即元素6A(3,[2,4]) % 第3行第2和第4列，即[10 12]A(1:3,3) % 第3列第1行至第4行，即[3;7;11]A(3,:) % 第3行所有元素，即[9 10 11 12]A(:,4) % 第4列所有元素，即[4;8;12] 矩阵变形 变换维度：通过函数reshape(X,m,n)实现：将中的数据按列取出，再根据指定的维度，从左至右按列填充 123456789101112t = 1:12;A = reshape(t, 2, 6)% A =% 1 3 5 7 9 11% 2 4 6 8 10 12B = reshape(A, 4, 3)% B =% 1 5 9% 2 6 10% 3 7 11% 4 8 12 也可以仅指定一个维度，让函数自适应另一个维度的大小 12C = reshape(B, 3, []); % 重构为3*4的矩阵D = reshape(C, [], 2); % 重构为6*2的矩阵 旋转与翻转 函数 说明 rot90(X) rot90(X,k) 将矩阵逆时针方向旋转 fliplr(X) 将矩阵左右翻转 flipud(X) 将矩阵上下翻转 flipdim(X,dim) dim=1时对行翻转，dim=2时对列翻转 三角阵/对角阵的抽取 提取对角线/用对角线构造对角阵 12345678910111213141516171819202122v = 1:3;A = diag(v) % 生成以向量v为对角线的对角阵% 1 0 0% A = 0 2 0% 0 0 3v = diag(A) % 提取矩阵A的对角线，生成列向量% 1% v = 2% 3A = diag(v,2) % 生成以向量v为主对角线上第2条对角线的对角阵% 0 0 1 0 0% 0 0 0 2 0% A = 0 0 0 0 3% 0 0 0 0 0% 0 0 0 0 0v = diag(A,2) % 提取主对角线上第2条对角线，生成列向% 1% v = 2% 3 提取上/下三角阵 123456789101112131415161718192021A = ones(3);tril(A) % 提取下三角阵% 1 0 0% 1 1 0% 1 1 1tril(A,1) % 从主对角线上第一条对角线开始提取下三角阵% 1 1 0% 1 1 1% 1 1 1triu(A) % 提取上三角阵% 1 1 1% 0 1 1% 0 0 1triu(A,-1) % 从主对角线下第一条对角线开始提取上三角阵% 1 1 1% 1 1 1% 0 1 1 几个函数虽然功能不同，但第二个参数中对角线的定位规则一致，正数代表主对角线上的对角线，负数代表下面的对角线，提取时取闭区间。 矩阵运算 加减运算 要求进行运算的矩阵形状一致（即各维度长度一致），计算时对应位置相加减即可，有交换律和结合律。 乘运算 若有三个矩阵，则对矩阵中的任意一个元素，有 即 其中矩阵的列数（第二维度）需要等于矩阵的行数（第一维度）。 矩阵乘运算不满足交换律。 点乘运算 两个形状一致的矩阵，对应位相乘，得到一个形状不变的矩阵。 除法运算 - 左除 线性方程组，若非奇异，即它的逆矩阵存在，则可解出X=inv(D)*B=D\\B 条件：的阶数等于的行数。（非奇异表明是方阵） 除法运算 - 右除 线性方程组，若非奇异，即它的逆矩阵存在，则可解出X=B*inv(D)=B/D 条件：的列数等于的阶数。 除号偏向哪边，哪边要求非奇异 使用除法运算解线性方程组比使用inv求逆的方法更迅速，且拥有更小的残差 常用矩阵运算函数 函数 说明 cond(A) 返回2-范数逆运算的条件数，即最大奇异值与最小奇异值之比 condest(A) 返回 1-范数条件数的下限 det(A) 返回矩阵的行列式（一个值） eig(A) 返回矩阵的特征值/特征向量 inv(A) 返回矩阵的逆 norm(A,p) 返回矩阵的范数，p可以为1 2 Inf 'fro'，不填默认为2 normest(A) 返回矩阵的2-范数，相当于norm(A,2) rank(A) 返回矩阵的秩 orth(A) 矩阵的正交化运算，返回适用于矩阵范围的标准正交基，列数等于秩 rcond(A) 返回1-范数的逆条件数 trace(A) 返回矩阵对角线之和，即矩阵的迹 expm(A) 矩阵指数运算，每个元素变为 logm(A) 矩阵对数运算，每个元素变为 sqrtm(A) 矩阵开方运算，返回使 cdf2rdf 将复数对角矩阵转换成实数块对角矩阵 rsf2csf 将实数块对角矩阵转换成复数对角矩阵 rref 将矩阵转换成逐行递减的阶梯矩阵 funm 一般的矩阵函数 关于矩阵的条件数与“病态”： 矩阵的条件数用于刻画矩阵的“病态”程度，定义为： 它是一个不小于1的实数，当时，说矩阵是“病态”的，反之则是“良态”的。 奇异值分解 SVD SVD分解是指将一个的矩阵表示为三个矩阵乘积的形式：，其中为阶方阵，为阶方阵，为阶对角阵，其对角线元素为矩阵的奇异值且满足： 其中为矩阵的秩。 12s = svd(A); % 返回矩阵A的奇异值列向量s[U,S,V] = svd(A); % 返回矩阵A的奇异值分解因子U、S和V","categories":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]},{"title":"Matlab元胞与结构体","slug":"matlab-basic-3","date":"2022-08-06T14:46:00.000Z","updated":"2022-08-06T15:06:09.966Z","comments":true,"path":"2022/08/06/matlab-basic-3/","link":"","permalink":"https://pyxblog.cn/2022/08/06/matlab-basic-3/","excerpt":"","text":"Matlab中的特殊变量允许用户将不同但相关的数据类型集成一个单一的变量，以便数据的管理，类似C++中的结构体。 单元型变量（元胞） 单元型变量是以“单元”为元素的数组，每个单元可以包含各种类型的数据（如矩阵、字符串），通过{}创建，通过下标直接引用。 数组类型为cell，其中每个元素的类型也为cell。 12345a = 1:10;b = 'test';c = [1+2i,1 1,1+2i];ce = {a, b, c}; 可以通过cell()函数预先分配空间，再对其中的元素进行逐个赋值。 指令 效果 cell(n) 生成阶空单元数组 cell(m,n)/cell([m,n]) 生成阶空单元数组 cell(m,n,p,...)/cell([...]) 生成阶空单元数组 cell(size(X)) 生成与矩阵同维的空单元数组 有关单元型变量的函数：可以通过lookfor cell查找学习 函数 说明 cellfun(func,C) 对单元型变量中的每个元素依次执行函数func celldisp(C) 在命令行中逐个输出每个元素的具体内容 cellplot(C) 用彩色图形窗口逐个显示元素的内容 num2cell(num) 将数值转换为单元型变量 deal 输入输出处理 cell2struct(C) 将单元型变量转换为结构型变量 struct2cell(St) 将结构型变量转换为单元型变量 iscell(X) 判断是否为单元型变量 reshape(X,[...]) 将中的元素按列取出，再按列重构为[]规定的维度 结构型变量 结构型变量是根据属性名field组织起来的不同数据类型的集合，每个属性可以包含不同的数据类型，如字符串、矩阵等，类似字典。通过函数struct来创建，通过属性名来引用属性值，通过索引来引用相应元素。 1234st = struct('name',{'Tom','Amy'}, 'sex',{'male','female'}, 'age',{18});st(1); % 每个属性的第一个值 name:'Tom', sex:'male', age:18st.sex; % 所有的sex属性 ans='male', ans='female'st(2).name; % name属性的第二个值 ans='Amy' 创建结构型变量时，要求每个属性的长度一致，或者为标量（只有一个值），如上述的name和sex长度一致，age是标量。 有关结构型变量的函数：可以通过lookfor struct查找学习 函数 说明 fieldnames(st) 返回结构型变量的所有属性名 getfield(st,fieldName) 返回指定属性名的所有属性值 setfield(st,fieldName,value) 设定指定属性名的值为value rmfield(st,fieldName) 删除指定属性 isfield(st,fieldName) 判断fieldName是不是st的属性 isstruct(st) 判断st是否是结构型变量","categories":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]},{"title":"Matlab向量与多项式","slug":"matlab-basic-2","date":"2022-08-05T14:51:28.000Z","updated":"2022-08-06T14:47:29.258Z","comments":true,"path":"2022/08/05/matlab-basic-2/","link":"","permalink":"https://pyxblog.cn/2022/08/05/matlab-basic-2/","excerpt":"","text":"向量是由个数组成的有序数组，记成或，叫做维向量，向量的第个分量称为。 向量生成与引用 向量生成 直接输入 向量用[]扩起来，元素之间用,或空格隔开，;则相当于换行 冒号法 t = 区间左端点 : 增量 : 区间右端点; 1234567t = first:increment:last;% first - 从first开始（闭区间）% last - 到last为止（闭区间）% increment - 增量，默认为1t = 0:2:10; % [0 2 4 6 8 10]t = 0:5; % [0 1 2 3 4 5]t = 10:-2:0; % [10 8 6 4 2 0] 利用函数linspace(区间左端点, 区间右端点, 个数) 123456linspace(first_value, last_value, number);% 在指定范围内等间隔采样% first_value - 从first_value开始（闭区间）% last_value - 到last_value结束（闭区间）% number - 包含number个元素x = linspace(0,10,5); % [0 2.5 5 7.5 10] 利用函数logspace(区间左端点, 区间右端点, 个数) 123456logspace(first_value, last_value, number);% 在指定范围内等间隔采样，但区间取10的幂% first_value - 从10^first_value开始（闭区间）% last_value - 到10^last_value结束（闭区间）% number - 包含number个元素x = logspace(1,3,3); % [10 100 1000] 可以同时采用多种方法创建向量，并使用[]将它们合并 12x = [2:4 4,5,6 linspace(10,20,2)]; % (2 3 4) (4 5 6) (10 20) 向量引用 格式 说明 x(index) 表示向量中的第个元素 x(i:j) 表示向量中的第到第个元素 x(i:delta:j) 表示向量中的第到第个元素，每个取一个值 向量的索引从1开始 可以用另一个向量作为索引，去访问向量，向量只能包含正整数，且不能超过的索引范围 12345x = 1:2:20;n1 = [1,4,9,7];n2 = 2:3:10; % [2,5,8]x(n1) % [1 7 17 13]x(n2) % [3 9 15] 向量运算 四则运算 相当于对向量中的元素分别进行四则运算 12345x = 2:2:10; % [2 4 6 8 10]x+1 % [3 5 7 9 11]x-1 % [1 3 5 7 9]x*2 % [4 8 12 16 20]x/2 % [1 2 3 4 5] 矩阵运算 向量可以看做特殊的矩阵，可以用矩阵运算的规则进行向量运算 1234567x = 2:2:10; % [2 4 6 8 10]y = 1:5; % [1 2 3 4 5]x+y % [3 6 9 12 15]x-y % [1 2 3 4 5]x.*y % [2 8 18 32 50] 按位乘x./y % [2 2 2 2 2] 按位除x*y' % 110 矩阵乘法 点积（数量积、内积） 向量和的点积定义为 其中，向量和向量必须长度相同。 123dot(a,b);sum(a.*b);sum(a*b'); 向量积（外积、叉积、叉乘） 向量和的向量积模长为 其方向与向量和所在平面垂直，且遵循右手定则。 12345cross(a,b); % 返回a和b的叉积，此时a和b必须是3维的向量% 计算其他维度叉积可以通过help cross查看对应方法x = [1,0,0];y = [0,1,0];z = cross(x,y); % z = [0,0,1] ps. 点积的结果是一个数，向量积的结果是一个同维向量 多项式 在Matlab中，用系数向量表示相应的多项式，以便进行多项式计算： 系数中的0不能省略，如 多项式的创建 使用系数向量p，通过函数poly2sym(p)创建多项式，返回sym类型多项式 1s = poly2sym([1,2,1]); % x^2 + 2*x + 1 使用根向量root，通过函数poly(root)创建多项式，返回系数向量 123root = [1 2];p = poly(root); % [1 -3 2]poly2sym(p) % x^2 - 3*x + 2 也可以编写字符串str，再通过eval(str)函数将其转换为sym类型。 sym类型表示“符号表达式”，也可以直接用于计算，后续的章节中包含相关内容。 多项式运算 多项式运算通过系数向量进行。 加减运算直接用+-实现，相加、相减的两个向量必须大小相等。 多项式乘法 相当于执行两个数组的卷积，用函数conv(p1,p2)实现，返回结果多项式的系数向量 123p1 = [1 2 1];p2 = [1 2 1];conv(p1,p2) % [1 4 6 4 1] 多项式除法 相当于执行两个数组的解卷，用函数deconv(p,q)实现，返回结果多项式的系数向量 123456789[k, r] = deconv(p, q);% k - p除以q的商% r - p除以q的余式% p = conv(q, k) + r;p1 = [1 2 1];p2 = [1 2 1];p = conv(p1,p2); % [1 4 6 4 1]deconv(p, p1) % [1 2 1] 多项式求导 通过函数polyder(p)实现，返回结果多项式的系数向量 12p = [1 1 1 1 1];polyder(p) % [4 3 2 1]","categories":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]},{"title":"Matlab基础知识","slug":"matlab-basic-1","date":"2022-08-05T14:19:06.000Z","updated":"2022-08-06T13:59:36.834Z","comments":true,"path":"2022/08/05/matlab-basic-1/","link":"","permalink":"https://pyxblog.cn/2022/08/05/matlab-basic-1/","excerpt":"","text":"常量 常量名称 对应含义 pi 圆周率 eps 浮点相对精度，常用于防止0出现在分母上 inf(Inf) 无穷大，如 NaN(nan) 不定值，如、、 i(j) 复数中的虚数单位 realmin 最小正浮点数 realmax 最大正浮点数 ps. 常量可以被赋值，使用命令clear+变量名或重启Matlab可以将常量恢复初始值。 数据类型 整型 含义 占用空间 char 字符型 1字节 unsigned char 无符号字符型 1字节 short 短整型 2字节 unsigned short 无符号短整型 2字节 int 有符号整型 4字节 unsigned int 无符号整型 4字节 long 长整型 4字节 unsigned long 无符号长整型 4字节 浮点型： 十进制数形式：由数字和小数点组成 指数形式：一般形式为 aEn，其中a为10进制数，n为10进制整数，表示 可以分为两类：单精度型和双精度型 单精度 float，占4字节，数值范围，最多7位有效数字 双精度 double，占8字节，数值范围，最多16位有效数字 ps. 使用命令format可以控制命令行的输出格式，参考help format 类型转换 12nu = 123;st = num2str(nu); % '123' 遇到不熟悉的类型转换时，可以使用lookfor指令查找相关函数： 123456789101112131415161718192021&gt;&gt; lookfor num2num2cell - Convert numeric array into cell array.num2hex - Convert singles and doubles to IEEE hexadecimal string formatnum2str - Convert numbers to character representationnum2ruler - Convert numeric array to ruler-appropriate array datatypeenum2val - Converts an enumerated string to its numerical equivalent.num2cell - Convert numeric codistributed array into cell arraynum2str - overloaded for gpuArraysnum2mstr - Convert number to string in maximum precision.iptnum2ordinal - Convert positive integer to ordinal string.num2ordinal - Convert positive integer to ordinal character vector.signal_num2str - Convert the number to a string.num2goid - Converts numbers to Gene Ontology IDs.num2str - Convert numbers to character representationdefnum2 - Sets Default channel namesnum2deriv - Numeric two-point network derivative function.num2base - Convert stored integers to stringsnum2sdec - Convert stored integers of array of fi objects to signed decimal representationnum2fixpt - Quantize a value using a Fixed-Point Designer representation.num2alphaheaders - Generate Alpha headers from a number (usually column).&gt;&gt; 运算符 算数运算符 定义 + 算数加 - 算数减 * 算数乘 .* 点乘 ^ 算数乘方 .^ 点乘方 \\ 算数左除 .\\ 点左除 / 算数右除 ./ 点右除 ' 矩阵共轭转置 .' 矩阵转置，不求共轭 其中： 左除a\\b表示，右除a/b表示 A*B表示矩阵乘法，要求矩阵的第二维度与矩阵的第一维度相等，A.*B表示矩阵按位乘法，要求矩阵和矩阵的形状相同，对乘方具有一致要求 A+B要求矩阵和矩阵的形状相同，对算数减有一致要求 A+a表示对矩阵中的每个元素都，对其他算数运算有一致要求 关系运算符 定义 == 等于 ~= 不等于 &gt;(&lt;) 大于/小于 &gt;=(&lt;=) 大于等于/小于等于 逻辑运算符 定义 &amp; 逻辑与 | 逻辑或 ~ 逻辑非 xor 逻辑异或 any 有非零元素则为真 all 所有元素均非零则为真 ps. 有关优先级 算数运算符 &gt; 关系运算符 &gt; 逻辑运算符 逻辑运算符中，~具有最高优先级，&amp;和|优先级相同 复数及三角运算函数 复数运算函数 对应含义 abs(z) 返回绝对值或复数的模 theta=angle(z) 返回复数的相位，范围 z=complex(a,b) 返回复数 conj(z) 返回复数的共轭 a=real(z) 返回复数的实部 b=imag(z) 返回复数的虚部 isreal(x) 判断矩阵是否含有复数 unwrap 平移相位角 cplxpair 将复数排序为复共轭对组 三角运算函数 对应含义 sin() 正弦函数 cos() 余弦函数 tan() 正切函数 cot() 余切函数 sec() 正割函数 csc() 余割函数 ps. 反三角函数=a+三角函数，如反正弦为asin()","categories":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]},{"title":"Matlab命令行","slug":"matlab-basic-0","date":"2022-08-05T13:34:25.000Z","updated":"2022-08-06T14:29:58.062Z","comments":true,"path":"2022/08/05/matlab-basic-0/","link":"","permalink":"https://pyxblog.cn/2022/08/05/matlab-basic-0/","excerpt":"","text":"常用指令 指令含义 指令 help+函数名 精确查询函数 lookfor+函数信息 模糊查询函数 who 内存变量列表 whos 内存变量详细信息 which + 文件名 查找文件位置 exist+变量名 判断变量是否存在 path 查看当前所有路径 addpath+路径 添加路径 clc 清除命令行 clear 清除内存变量 close all 关闭所有窗口 应用示例 使用 help 指令查询函数用法 如同所示，使用 help 指令查询已知函数名的任意函数用法，其中大部分常用函数均有官方的中文版用法详解及相关实例，少部分尚未完成汉化（英文版文档），但也可以通过实例看懂。 使用 lookfor 指令查找相关函数 如同所示，使用 help 指令查询与 plot 相关的函数，会输出函数名/函数描述中带有\"plot\"字段的所有结果以供查看。 一般情况下，在主函数中前三行会写： 123clearclose allclc 作用是在主函数最开始依次执行： 清除内存变量 关闭之前打开的图形窗口 清空命令行（上一次运行代码的输入输出） 大家可以养成习惯哦！","categories":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/"},{"name":"实战","slug":"爬虫/实战","permalink":"https://pyxblog.cn/categories/%E7%88%AC%E8%99%AB/%E5%AE%9E%E6%88%98/"},{"name":"markdown系列教程","slug":"markdown系列教程","permalink":"https://pyxblog.cn/categories/markdown%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/"},{"name":"深度学习","slug":"深度学习","permalink":"https://pyxblog.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"实战demo","slug":"深度学习/实战demo","permalink":"https://pyxblog.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E6%88%98demo/"},{"name":"python","slug":"python","permalink":"https://pyxblog.cn/categories/python/"},{"name":"基础教程","slug":"python/基础教程","permalink":"https://pyxblog.cn/categories/python/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"},{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/categories/Matlab/"},{"name":"基础教程","slug":"Matlab/基础教程","permalink":"https://pyxblog.cn/categories/Matlab/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"python","slug":"python","permalink":"https://pyxblog.cn/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://pyxblog.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"scrapy","slug":"scrapy","permalink":"https://pyxblog.cn/tags/scrapy/"},{"name":"latex","slug":"latex","permalink":"https://pyxblog.cn/tags/latex/"},{"name":"markdown","slug":"markdown","permalink":"https://pyxblog.cn/tags/markdown/"},{"name":"PyTorch","slug":"PyTorch","permalink":"https://pyxblog.cn/tags/PyTorch/"},{"name":"深度学习","slug":"深度学习","permalink":"https://pyxblog.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Matlab","slug":"Matlab","permalink":"https://pyxblog.cn/tags/Matlab/"}]}